---
layout: post
title: "사이킷런"
date: 2021-02-22
author: 단우아범
categories: "머신러닝"
tags:	
cover: "/assets/instacode.png"
---

# 0. Scikit-Learn 소개 
가장 유명한 파이썬의 머신러닝 알고리즘 라이브러리  
사이킷런에서의 데이터 표현 방식을 이해하는 것이 중요한데, 사이킷런에서는 데이터를 데이터 테이블 관점으로 이해하는 것이 중요하다.  
테이블 관점이란, 행과 열로 존재하는 것이며, 행(row)을 표본(samples)이라고 하고, 행의 개수를 n_samples라고 표현함  
열(col)은 특징(feature)라고 하고, 열의 개수를 n_features라고 표현함  

- 특징 배열  
[n_samples, n_features] 모양을 가진 2차원 행렬  
```
x_iris = iris.drop('species', axis=1)
```

- 대상 벡터  
[n_samples, n_target] 모양을 가진 nx1 2차원 행렬
```
y_iris = iris['species']
```



# 1. 사이킷런 API  
사이킷런 Estimator APi를 이용하는 단계는 아래와 같음  
1. 사이킷런으로부터 적정한 Estimator(추정기) 클래스를 임포트해서 모델의 클래스를 선택함  
2. 이 클래스를 원하는 값으로 인스턴스화해서 모델의 하이퍼파라미터를 선택함  
3. 데이터를 특징 배열과 대상 벡터로 배치함  
4. 모델 인스턴스의 fit() 메서드를 호출해 모델을 데이터에 적합시킴  
5. 모델을 새 데이터에 적용함  
  - 지도 학습 : 대체로 predict() 메서드를 사용해 test셋을 적용시킴  
  - 비지도 학습 : transform()이나 predict()를 사용해 데이터 속성을 변환하거나 추론함  
  - 예시  
```
# 1. 모델의 클래스 선택
from sklearn.linear_model import LinearRegression

# 2. 모델의 하이퍼파라미터 선택
# 모델 클래스가 모델 인스턴스와 같지 않음  
model = LinearRegression(~~~)

# 3. 데이터를 특징 배열과 대상 백터로 배치함  

# 4. 모델을 데이터에 적합시킴  
model.fit(X,y)  
# fit() 명령어에는 모델에 종속된 여러 가지 내부 계산이 되따르며, 계산된 결과는 모델 전용 속성에 저장되어 탐색할 수 있음  
# 밑줄 표시(_)가 붙는 것이 특징임  
# e.g. model.coef_, model.intercept_


# 5. 새 데이터에 적용함

```




# 2. 핸즈온머신러닝_2_ch2  
1. 데이터 나누기  
  ```
  from sklearn.model_selection import train_test_split

  train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
  ```
 - 순수한 무작위 샘플리으 방식은 위와 같음  
 - 데이터 셋이 충분히 크다면 (특성 수에 비해) 일반적으로 괜찮음  
 - 그렇지 않다면, 샘플링 편향이 생길 수 있음  
 - 예를들어, 소득 카테고리의 비율을 고려해서 샘플링 등을 하고 싶을 수 있음  
 
 ```
 housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])
 
 from sklearn.model_selection import StratifiedShuffleSplit

  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) #n_splits는 나눌 샘플의 개수 ex_ n_splits = 3이면 train, test 3쌍씩 만듬  
  for train_index, test_index in split.split(housing, housing["income_cat"]):
      strat_train_set = housing.loc[train_index]
      strat_test_set = housing.loc[test_index]
 
 ```
 - np.cut()을 이용해 구간화하고, StratifiedShuffleSplit을 이용해서 구간의 비율을 고려해 샘플링함  


2. 데이터 이해를 위한 탐색과 시각화  
  ```
  housing = strat_train_set.copy() # copy해서 사용하는 것을 추천함
  
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
             s=housing["population"]/100, label="population", figsize=(10,7),
             c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
             sharex=False)
             # 산점도
             #s는 크기, c는 색깔, cmap은 색깔 범위표시, sharex는 x-축의 값과 범례를 표시하지 못하는 버그를 수정함
             
  plt.legend()
  
  ```
  
  ```
  # 상관관계 확인하기
  
  corr_matrix = housing.corr()
  
  # from pandas.tools.plotting import scatter_matrix # 옛날 버전의 판다스에서는
  # 판다스 scatter_matrix로 산점도 & 히스토그램을 그려줌
  from pandas.plotting import scatter_matrix

  attributes = ["median_house_value", "median_income", "total_rooms",
                "housing_median_age"]
  scatter_matrix(housing[attributes], figsize=(12, 8))
  save_fig("scatter_matrix_plot")
  ```
  
  ```
  # 아이디어를 토대로 새로운 파생변수 생성하기
  housing["rooms_per_household"] = housing["total_rooms"]/housing["households"]
  housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"]
  housing["population_per_household"]=housing["population"]/housing["households"]
  ```

3. 머신러닝 알고리즘을 위한 데이터 준비  
데이터 전처리는 꼭 함수화하여 자동화하는 습관을 들이는 것이 중요하다!!!!!  
  ```
  # X, Y변수 분리
  housing = strat_train_set.drop("median_house_value", axis=1) # 훈련 세트를 위해 레이블 삭제
  housing_labels = strat_train_set["median_house_value"].copy()
  
  # null처리
  sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()
  
  # 삭제 or 전체삭제 or 대체
  sample_incomplete_rows.dropna(subset=["total_bedrooms"])    # 옵션 1
  sample_incomplete_rows.drop("total_bedrooms", axis=1)       # 옵션 2
  median = housing["total_bedrooms"].median()
  sample_incomplete_rows["total_bedrooms"].fillna(median, inplace=True) # 옵션 3
  
  #사이킷런의 SimpleImputer는 누락된 값을 손쉽게 다루게 도와줌
  from sklearn.impute import SimpleImputer
  imputer = SimpleImputer(strategy="median")
  
  # 중간값은 수치형 특성에서만 계산될 수 있기 때문에 텍스트 특성을 제거함
  housing_num = housing.drop("ocean_proximity", axis=1)
  # 다른 방법: housing_num = housing.select_dtypes(include=[np.number])
  imputer.fit(housing_num)
  imputer.statistics_
  
  # imputer를 활용해서 훈련 세트를 변환함
  X = imputer.transform(housing_num)
  housing_tr = pd.DataFrame(X, columns=housing_num.columns,
                          index=housing_num.index)
  imputer.strategy
  
  #범주형 입력 특성 전처리 ▶ 숫자로
  
  # 순서가 의미가 있는 경우는 OrdinalEncoder(좋음, 나쁨, 매우나쁨 등)
  housing_cat = housing[["ocean_proximity"]]
  housing_cat.head(10)
  
  from sklearn.preprocessing import OrdinalEncoder

  ordinal_encoder = OrdinalEncoder()
  housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
  housing_cat_encoded[:10]
  
  #나머지는 원핫인코딩
  from sklearn.preprocessing import OneHotEncoder

  cat_encoder = OneHotEncoder()
  housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
  housing_cat_1hot
  # 원핫인코더는 기본적으로 희소행렬을 반환하여, 필요시 toarray()를 사용해 밀집 배열로 변환가능함
  housing_cat_1hot.toarray()
  
  ## 또는 sparse=False로 바로 밀집 배열로 만들 수 있으나, 메모리가 낭비됨
  cat_encoder = OneHotEncoder(sparse=False)
  housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
  housing_cat_1hot
  
  ```
  
  - 나만의 변환기를 만들어야 할 경우가 많음  
  ```
  from sklearn.base import BaseEstimator, TransformerMixin
  # 열 인덱스
  #rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6
  col_names = "total_rooms", "total_bedrooms", "population", "households"
  rooms_ix, bedrooms_ix, population_ix, households_ix = [
    housing.columns.get_loc(c) for c in col_names] # 열 인덱스 구하기


  class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
      def __init__(self, add_bedrooms_per_room=True): # *args 또는 **kargs 없음
          self.add_bedrooms_per_room = add_bedrooms_per_room
      def fit(self, X, y=None):
          return self  # 아무것도 하지 않습니다
      def transform(self, X):
          rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
          population_per_household = X[:, population_ix] / X[:, households_ix]
          if self.add_bedrooms_per_room:
              bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
              return np.c_[X, rooms_per_household, population_per_household,
                           bedrooms_per_room]
          else:
              return np.c_[X, rooms_per_household, population_per_household]

  attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
  housing_extra_attribs = attr_adder.transform(housing.to_numpy())
  
  housing_extra_attribs = pd.DataFrame(
    housing_extra_attribs,
    columns=list(housing.columns)+["rooms_per_household", "population_per_household"],
    index=housing.index)
  housing_extra_attribs.head()
  
  ```
  
  - 가장 중요한 Feature Scaling  
  - 일반적으로, 머신러닝 알고리즘은 입력 숫자 특성들의 스케일이 많이 다르면 잘 작동하지 않음  
  - 정규화(min-max)나 표준화가 가장 많이 사용됨  
  - 표준화는 먼저 평균을 뺀 후, 표준편차로 나누어 결과 분포의 분산이 1이 되도록 함  
  - 정규화와 달리 범위의 상한과 하한이 없어 어떤 알고리즘에서는 문제가 될 수 있으나, 정규화보다 이상치 영향을 덜 받음 
  - 가장 중요한 것은, 스케일링은 train_set에 대해서만 fit()해서 적용시키고, 훈련세트와 테스트 세트에 transform()을 사용함   
  ```
  
  from sklearn.pipeline import Pipeline
  from sklearn.preprocessing import StandardScaler

  num_pipeline = Pipeline([
          ('imputer', SimpleImputer(strategy="median")),
          ('attribs_adder', CombinedAttributesAdder()),
          ('std_scaler', StandardScaler()),
      ])

  housing_num_tr = num_pipeline.fit_transform(housing_num)
  
  
  from sklearn.compose import ColumnTransformer

  num_attribs = list(housing_num)
  cat_attribs = ["ocean_proximity"]

  full_pipeline = ColumnTransformer([
          ("num", num_pipeline, num_attribs),
          ("cat", OneHotEncoder(), cat_attribs),
      ])

  housing_prepared = full_pipeline.fit_transform(housing)
  
  ```
5. 모델 선택과 훈련  

  ```
  # 훈련 샘플 몇 개를 사용해 전체 파이프라인을 적용해 보겠습니다
  some_data = housing.iloc[:5]
  some_labels = housing_labels.iloc[:5]
  some_data_prepared = full_pipeline.transform(some_data)

  print("예측:", lin_reg.predict(some_data_prepared))
  ```
6. 모델 세부 튜닝  
  ```
  from sklearn.model_selection import cross_val_score

  scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                           scoring="neg_mean_squared_error", cv=10)
  tree_rmse_scores = np.sqrt(-scores)
  
  from sklearn.model_selection import GridSearchCV

param_grid = [
    # 12(=3×4)개의 하이퍼파라미터 조합을 시도합니다.
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    # bootstrap은 False로 하고 6(=2×3)개의 조합을 시도합니다.
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]

  forest_reg = RandomForestRegressor(random_state=42)
  # 다섯 개의 폴드로 훈련하면 총 (12+6)*5=90번의 훈련이 일어납니다.
  grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                             scoring='neg_mean_squared_error',
                             return_train_score=True)
  grid_search.fit(housing_prepared, housing_labels)
  
  grid_search.best_params_
  cvres = grid_search.cv_results_
  for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
      print(np.sqrt(-mean_score), params)
      
  from sklearn.model_selection import RandomizedSearchCV
  from scipy.stats import randint

  param_distribs = {
          'n_estimators': randint(low=1, high=200),
          'max_features': randint(low=1, high=8),
      }

  forest_reg = RandomForestRegressor(random_state=42)
  rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,
                                  n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
  rnd_search.fit(housing_prepared, housing_labels)    
  cvres = rnd_search.cv_results_
  for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
  print(np.sqrt(-mean_score), params)
    
  feature_importances = grid_search.best_estimator_.feature_importances_
  feature_importances
  ```
8. 추가 내용  

  ```
  # 전처리와 예측을 포함한 전체 파이프라인
  full_pipeline_with_predictor = Pipeline([
          ("preparation", full_pipeline),
          ("linear", LinearRegression())
      ])

  full_pipeline_with_predictor.fit(housing, housing_labels)
  full_pipeline_with_predictor.predict(some_data)

  # Joblib을 사용한 모델 저장
  my_model = full_pipeline_with_predictor
  import joblib
  joblib.dump(my_model, "my_model.pkl") # DIFF
  #...
  my_model_loaded = joblib.load("my_model.pkl") # DIFF
  ```
