---
layout: post
title: "모델별_Parameter_처리"
date: 2021-01-12
author: 단우아범
categories: "머신러닝"
tags:	
cover: "/assets/instacode.png"
---


# 0. 들어가며  
머신러닝에서 데이터와 분석목표에 맞게 모델을 선택하는 것을 한 후, Parameter를 가공해야 하는 법에 대해 자신이 없다.  
스케일링을 해야 하는지, 어떤 것으로 해야 하는지, 변수들은 어떻게 만들어 줘야 하는지 등등..  

데이터 전처리는 어떻게 하는지, 모델별 특징은 어떠한지, 하이퍼 파라미터는 어떻게 튜닝해야 하는지..  
아래와 같이 정리해보자!  

# 1. 데이터 전처리  

## 1.1 데이터 인코딩  
[**참고**](https://2-chae.github.io/category/1.ai/30#:~:text=%EC%82%AC%EC%9A%A9%ED%95%B4%EB%8F%84%20%EB%90%9C%EB%8B%A4.-,One%2Dhot%20Encoding,%EC%9D%84%20%ED%91%9C%EC%8B%9C%ED%95%98%EB%8A%94%20%EB%B0%A9%EC%8B%9D%EC%9D%B4%EB%8B%A4.)  

사이킷런의 머신러닝 알고리즘은 문자열 값을 입력으로 허용하지 않는다.  
그래서 이 문자열을 숫자형으로 변환해야하는데 이때 문자열 피처는 일반적으로 카테고리형과 텍스트형을 의미한다.  
카테고리형 피처는 코드 값으로 표현될 수 있으며 텍스트형 피처는 피처 벡터화 등의 기법으로 벡터화하거나 불필요하다면 삭제하는게 좋다.  

### 1.1.1 라벨 인코딩  
    레이블 인코딩은 간단하게 문자열 값을 숫자형 카테고리 값으로 변환한다.  
    그러나 일괄적인 숫자값으로의 변환이 예측성능을 떨어트리는 원인이 될 수 있다.  
    이는 할당받은 카테고리 별 숫자 값의 크고 작음에 대한 특성이 작용하기 때문이다.  
    냉장고 1 이 믹서 2보다 작은 값을 가지므로 믹서가 냉장고보다 특정 ML알고리즘에서 가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생한다.  
    이러한 특성때문에 레이블 인코딩은 선형회귀와 같은 ML 알고리즘에서는 적용하지 않아야한다.  
    트리 계열의 ML 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩을 사용해도 된다.  
    
### 1.1.2 원핫 인코딩
    원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식이다.  
    원-핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가하고 고유 값에 해당하는 칼럼에만 1을 표시, 나머지는 다 0을 표시하는 방식이다.  
    즉, 행 형태로 되어 있는 피처의 고유값을 열 형태로 차원을 변환한 뒤, 고유 값에 해당하는 칼럼에만 2을 표시하고 나머지 칼럼에는 0을 표시한다.  
    
    원-핫 인코딩은 사이킷런에 OneHotEncoder 클래스로 쉽게 변환이 가능하다.  
    주의할 점이 있는데 첫 번째는 OneHotEncoder로 변환하기 전에 모든 문자열 값이 숫자형 값으로 변환돼야 하며, 두 번째는 입력 값으로 2차원 데이터가 필요하다는 점이다.  
    LabelEncoder를 거쳐서 바꾸는게 용이하다.


# 2. 하이퍼 파라미터 튜닝 수행 방법  

## 2.0 주요 이슈  
- 하이퍼 파라미터 개수가 많고, 범위가 넓을 경우 오랜 시간이 소요됨  
- 



======================

# 2. 분류  
## 2.1 Tree 계열  
Tree기반 Gradient Boosting 방식 알고리즘이 모델 성능의 우수성은 인정되나, 학습 시간이 너무 오래 걸리는 문제가 존재함  

### 2.1.1 XGBoost  
#### [정의]  
2016년에 발표된 이후 더욱 더 Gradient Boosting 방식이 인기를 얻음  
모델의 성능은 물론 병렬 학습이 가능하여 기존 방식보다 학습 시간을 대폭 단축함  
대용량 데이터에 대해서는 학습시간이 오래 걸림  
       
#### [하이퍼 파라미터]  
      
      
### 2.1.2 LGBM  
#### [정의]  
대용량 데이터에 최적화 모델을 만들기 위해 반복적인 Feature Engineering과 하이퍼 파라미터 튜닝을 수행해야 하므로 상대적으로 많이 활용됨  
타 Tree 계열이 level_wise(수평적 확장)일 때, LGBM은 leaf-wise(수직적 확장)됨  
<img src = "https://user-images.githubusercontent.com/59005950/104281477-eb35fc00-54f0-11eb-9902-4d156bac82c3.jpg">  
level_wise보다 overfitting하기 쉬워 작은 데이터 세트에 사용하는 것은 추천되지 않음. 최소 10,000개 row 이상  
이런 단점을 하이퍼 파라미터와 구현 기술로 극복하여 XGBoost 대비 학습시간은 4배 이ㅏㅇ 빠르지만, 성능은 대등함  
특히, feature 개수가 수백~수천개가 되어도 성능이 크게 저하되지 않고 뛰어난 성능을 여전히 나타냄  
확장을 위해 max delta loss를 가진 leaf를 선택하게 되어 동일 leaf를 확장할 때 더 많은 손실을 줄일 수 있음  
[**참고**](https://nurilee.com/lightgbm-definition-parameter-tuning/)  

#### [하이퍼 파라미터] 
- Object 변수들을 Encoding 할때, one-hot-encoding이든 label_encoding이든 성능 면에서 큰 차이가 없다.  
- Boosting_type은 gbdt(default), goss, dart, rt가 있으며, 어떤 type이 최적인지는 데이터 세트/모델에 따라 달라질 수 있으므로 `직접 수행을 해봐야 알 수 있음`  
  - GOSS(Gradient-based One-Side Sampling) : Gradient 값이 상대적으로 큰 값에 대해서만 선택적으로 필터링하여 반복적 재학습  
  - 수행 시간을 향상시키지만, 성능은 향상시키지 않음  
  - DART(Dropouts meet Multiple Additive Regression Trees) : Iteration을 지속적으로 수행하면서 추가적 트리가 만들어 질 때 마지막에 만들어지는 트리들이 일부 데이터 세트 조건만을 만들어지는데 이를 아예 Drop Out함  
- max_depth, num_leaves를 조정한다  
<img src = "https://user-images.githubusercontent.com/59005950/104284649-b1b3bf80-54f5-11eb-908e-751207a2a0b1.jpg">  
  - max_depth는 보통 10 ~ 16  
  - num_leaves는 20 ~ 40  등 해봐야 안다  
<img src = "https://user-images.githubusercontent.com/59005950/104285079-6bab2b80-54f6-11eb-8b4d-f586eda17570.jpg">  

#### [하이퍼 파라미터 튜닝] 
- Grid Search  
  - grid : 격자 ▶ [ 1, 2, 3, 4] 순차적으로 하이퍼파라미터를 결합해서 다 확인 ▶ 시간이 오래걸림  
- Random Search  
  - 랜덤으로 확인함  ▶ 사용자 운빨, 경험적 요소에 많이 좌우됨  
- Bayesian Optimization  
  - Gaussian Process를 통해 함수의 사후 분포를 생성하고, 이를 기반으로 최적화하려는 함수를 재구성함  
  - 점차 많은 입력값을 받아서 사후 분포가 개선되고, 함수 반환 값을 최대(최소)가 되는 입력 파라미터 영역을 보다 확실하게 찾게 됨  

  ```
  from bayes_opt import BayesianOptimization
  from sklearn.metrics import roc_auc_score
  from lightgbm import LGBMClassifier
  
  # parameter 별로 search할 범위를 설정. 
  bayesian_params = {
      'max_depth': (6, 16), 
      'num_leaves': (24, 64), 
      'min_child_samples': (10, 200), 
      'min_child_weight':(1, 50),
      'subsample':(0.5, 1.0),
      'colsample_bytree': (0.5, 1.0),
      'max_bin':(10, 500),
      'reg_lambda':(0.001, 10),
      'reg_alpha': (0.01, 50) 
  }
  
  def lgb_roc_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample, 
                colsample_bytree,max_bin, reg_lambda, reg_alpha):
    params = {
        "n_estimators":500, "learning_rate":0.02,
        'max_depth': int(round(max_depth)), #  호출 시 실수형 값이 들어오므로 정수형 하이퍼 파라미터는 정수형으로 변경 
        'num_leaves': int(round(num_leaves)), 
        'min_child_samples': int(round(min_child_samples)),
        'min_child_weight': int(round(min_child_weight)),
        'subsample': max(min(subsample, 1), 0), 
        'colsample_bytree': max(min(colsample_bytree, 1), 0),
        'max_bin':  max(int(round(max_bin)),10),
        'reg_lambda': max(reg_lambda,0),
        'reg_alpha': max(reg_alpha, 0)
    }
    lgb_model = LGBMClassifier(**params)
    lgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 100, 
                early_stopping_rounds= 100)
    valid_proba = lgb_model.predict_proba(valid_x)[:, 1]
    roc_auc = roc_auc_score(valid_y, valid_proba)
    
    return roc_auc   
    
    
  # BayesianOptimization객체를 수행할 함수와 search할 parameter 범위를 설정하여 생성. 
  lgbBO = BayesianOptimization(lgb_roc_eval,bayesian_params , random_state=0)
  # 함수 반환값이 최대가 되는 입력값 유추를 위한 iteration 수행. 
  lgbBO.maximize(init_points=5, n_iter=25)
    
  ```
### 2.1.3 GBM(Gradient Boosting Machine) 
#### [정의]   
Gradient Descent + Boosting  


#### [하이퍼 파라미터]  
- 
    
======================    
    
# 회귀
  - 
