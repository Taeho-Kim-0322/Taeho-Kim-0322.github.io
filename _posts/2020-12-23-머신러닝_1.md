---
layout: post
title: "모델별_Parameter_처리"
date: 2021-01-12
author: 단우아범
categories: "머신러닝"
tags:	
cover: "/assets/instacode.png"
---


# 0. 들어가며  
머신러닝에서 데이터와 분석목표에 맞게 모델을 선택하는 것을 한 후, Parameter를 가공해야 하는 법에 대해 자신이 없다.  
스케일링을 해야 하는지, 어떤 것으로 해야 하는지, 변수들은 어떻게 만들어 줘야 하는지 등등..  

데이터 전처리는 어떻게 하는지, 모델별 특징은 어떠한지, 하이퍼 파라미터는 어떻게 튜닝해야 하는지..  
아래와 같이 정리해보자!  

# 1. 데이터 전처리  

## 1.1 데이터 인코딩  
[**참고**](https://2-chae.github.io/category/1.ai/30#:~:text=%EC%82%AC%EC%9A%A9%ED%95%B4%EB%8F%84%20%EB%90%9C%EB%8B%A4.-,One%2Dhot%20Encoding,%EC%9D%84%20%ED%91%9C%EC%8B%9C%ED%95%98%EB%8A%94%20%EB%B0%A9%EC%8B%9D%EC%9D%B4%EB%8B%A4.)  

사이킷런의 머신러닝 알고리즘은 문자열 값을 입력으로 허용하지 않는다.  
그래서 이 문자열을 숫자형으로 변환해야하는데 이때 문자열 피처는 일반적으로 카테고리형과 텍스트형을 의미한다.  
카테고리형 피처는 코드 값으로 표현될 수 있으며 텍스트형 피처는 피처 벡터화 등의 기법으로 벡터화하거나 불필요하다면 삭제하는게 좋다.  

### 1.1.1 라벨 인코딩  
    레이블 인코딩은 간단하게 문자열 값을 숫자형 카테고리 값으로 변환한다.  
    그러나 일괄적인 숫자값으로의 변환이 예측성능을 떨어트리는 원인이 될 수 있다.  
    이는 할당받은 카테고리 별 숫자 값의 크고 작음에 대한 특성이 작용하기 때문이다.  
    냉장고 1 이 믹서 2보다 작은 값을 가지므로 믹서가 냉장고보다 특정 ML알고리즘에서 가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생한다.  
    이러한 특성때문에 레이블 인코딩은 선형회귀와 같은 ML 알고리즘에서는 적용하지 않아야한다.  
    트리 계열의 ML 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩을 사용해도 된다.  
    
### 1.1.2 원핫 인코딩
    원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식이다.  
    원-핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가하고 고유 값에 해당하는 칼럼에만 1을 표시, 나머지는 다 0을 표시하는 방식이다.  
    즉, 행 형태로 되어 있는 피처의 고유값을 열 형태로 차원을 변환한 뒤, 고유 값에 해당하는 칼럼에만 2을 표시하고 나머지 칼럼에는 0을 표시한다.  
    
    원-핫 인코딩은 사이킷런에 OneHotEncoder 클래스로 쉽게 변환이 가능하다.  
    주의할 점이 있는데 첫 번째는 OneHotEncoder로 변환하기 전에 모든 문자열 값이 숫자형 값으로 변환돼야 하며, 두 번째는 입력 값으로 2차원 데이터가 필요하다는 점이다.  
    LabelEncoder를 거쳐서 바꾸는게 용이하다.

======================

# 2. 분류  
## 2.1 Tree 계열  
    - Tree기반 Gradient Boosting 방식 알고리즘이 모델 성능의 우수성은 인정되나, 학습 시간이 너무 오래 걸리는 문제가 존재함  
### 2.1.1 XGBoost  
#### [정의]  
2016년에 발표된 이후 더욱 더 Gradient Boosting 방식이 인기를 얻음  
모델의 성능은 물론 병렬 학습이 가능하여 기존 방식보다 학습 시간을 대폭 단축함  
대용량 데이터에 대해서는 학습시간이 오래 걸림  
       
#### [하이퍼 파라미터]  
      
      
### 2.1.2 LGBM  
#### [정의]  
대용량 데이터에 최적화 모델을 만들기 위해 반복적인 Feature Engineering과 하이퍼 파라미터 튜닝을 수행해야 하므로 상대적으로 많이 활용됨  
타 Tree 계열이 level_wise(수평적 확장)일 때, LGBM은 leaf-wise(수직적 확장)됨  
<img src = "https://user-images.githubusercontent.com/59005950/104281477-eb35fc00-54f0-11eb-9902-4d156bac82c3.jpg">  
level_wise보다 overfitting하기 쉬워 작은 데이터 세트에 사용하는 것은 추천되지 않음. 최소 10,000개 row 이상  
이런 단점을 하이퍼 파라미터와 구현 기술로 극복하여 XGBoost 대비 학습시간은 4배 이ㅏㅇ 빠르지만, 성능은 대등함  
특히, feature 개수가 수백~수천개가 되어도 성능이 크게 저하되지 않고 뛰어난 성능을 여전히 나타냄  
확장을 위해 max delta loss를 가진 leaf를 선택하게 되어 동일 leaf를 확장할 때 더 많은 손실을 줄일 수 있음  
[**참고**](https://nurilee.com/lightgbm-definition-parameter-tuning/)  

#### [하이퍼 파라미터] 
- Object 변수들을 Encoding 할때, one-hot-encoding이든 label_encoding이든 성능 면에서 큰 차이가 없다.  
- 
      
### 2.1.3 GBM(Gradient Boosting Machine) 
#### [정의]   
Gradient Descent + Boosting  

      
#### [하이퍼 파라미터]  
- 
    
======================    
    
# 회귀
  - 
