---
layout: post
title:  "NLP 3일차 : BERT"
date:   2020-12-03
author: 단우아범
categories: "언어지능"
tags:	
cover:  "/assets/instacode.png"
---

# 2-1 BERT 넌 누구니
## 1. BERT란
 BERT : Bidirectional Encoder Representations from Transformers  
 - Model의 특징
  - Bi-directional
  - Transformer 구조 이용  
 - 사전학습 과제 수행 후 Fine-tuning
  - Masked Language Model
  - Next Sentence Prediction  
  
 BERT 사전학습 과제 1 : Masked Language Model
  - 가려진 단어를 맞추는 과제를 해결함으로써 주변 맥락에 따른 의미 학습
  - 자연어 토큰의 의미를 인코딩하는 방법을 학습함
  - Transformer Encoder 구조를 이용함
  
 BERT 사전학습 과제 2 : Next Sentence Prediction
  - 제시된 두 문장이 이어진 문장인지 아닌지를 맞추는 과제를 수행
  - 문맥의 일관성을 학습함
 
 BERT Fine-tuning : 주어진 과제 유형에 따라서 마지막 output layer만 변경하여 간단하게 fine-tuning함  
 
 BERT 요약 : Self-supervised learning : 방대한 양의 데이터로 수행한 사전학습 과제의 힘
  - 사전 학습 과제
    - 엄청난 자원을 사용하여 4일 내내 학습함
  - Fine-tuning
    - 태스크에 따라 1~4 epoch만 추가 수행
 
---
 
# 2-2 BERT 토크나이징
## 1. BPE, Wordpiece Tokenization
 UNK가 많이 생기지 않으면서도 필요한 경우 의미를 가지는 토큰 단위를 만들어내는 방법, Byte Pair Encoding과 WordPiece Tokenization  
 
### Byte Pair Encoding (BPE)  
 BPE는 __빈도수가 높은 문자의 조합__ 으로 문장을 파싱하는 방법  
### WordPiece Tokeniztion  
 BPE와 거의 비슷하지만, 빈도수 대신 단어의 likelihood가 높은 조합을 찾아낸다는 것만 차이가 있음  


## 2. Google Multi-lingual BERT
 구글에서 한국어를 포함한 102국어를 지원하는 Multi-lingual BERT 모델을 공개함
 
 
