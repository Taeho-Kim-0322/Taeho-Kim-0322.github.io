---
layout: post
title:  "NLP 3일차 : BERT"
date:   2020-12-03
author: 단우아범
categories: "언어지능"
tags:	
cover:  "/assets/instacode.png"
---

# 2-1 BERT 넌 누구니
## 1. BERT란
 BERT : Bidirectional Encoder Representations from Transformers  
 - Model의 특징
  - Bi-directional
  - Transformer 구조 이용  
 - 사전학습 과제 수행 후 Fine-tuning
  - Masked Language Model
  - Next Sentence Prediction  
  
 BERT 사전학습 과제 1 : Masked Language Model
  - 가려진 단어를 맞추는 과제를 해결함으로써 주변 맥락에 따른 의미 학습
  - 자연어 토큰의 의미를 인코딩하는 방법을 학습함
  - Transformer Encoder 구조를 이용함
  
 BERT 사전학습 과제 2 : Next Sentence Prediction
  - 제시된 두 문장이 이어진 문장인지 아닌지를 맞추는 과제를 수행
  - 문맥의 일관성을 학습함
 
 BERT Fine-tuning : 주어진 과제 유형에 따라서 마지막 output layer만 변경하여 간단하게 fine-tuning함  
 
 BERT 요약 : Self-supervised learning : 방대한 양의 데이터로 수행한 사전학습 과제의 힘
  - 사전 학습 과제
    - 엄청난 자원을 사용하여 4일 내내 학습함
  - Fine-tuning
    - 태스크에 따라 1~4 epoch만 추가 수행
 
---
 
# 2-2 BERT 토크나이징
## 1. BPE, Wordpiece Tokenization
 UNK가 많이 생기지 않으면서도 필요한 경우 의미를 가지는 토큰 단위를 만들어내는 방법, Byte Pair Encoding과 WordPiece Tokenization  
 
### Byte Pair Encoding (BPE)  
 BPE는 __빈도수가 높은 문자의 조합__ 으로 문장을 파싱하는 방법  
### WordPiece Tokeniztion  
 BPE와 거의 비슷하지만, 빈도수 대신 단어의 likelihood가 높은 조합을 찾아낸다는 것만 차이가 있음  


## 2. Google Multi-lingual BERT
 구글에서 한국어를 포함한 102국어를 지원하는 Multi-lingual BERT 모델을 공개함
 
 
---
 
# 3-1 BERT 인풋들
## 1. BERT 인풋
BERT는 input_ids, segment_ids, input_masks라는 3가지 input layer를 대입함  
  1. input_ids : 인풋 텍스트를 단어사전을 이용해 id로 변환한 리스트  
    텍스트는 스페셜 토큰 [CLS] 에 해당하는 id로 시작함  
    속성이 다른 텍스트 사이 혹은 인풋이 끝났을 때는 [SEP] 토큰 사용  
  2. Segment_ids : 텍스트 분석의 단위를 0 혹은 1로 나타내주기 위한 id  
    예를 들어 두 개의 문장의 유사도를 측정하는 태스크라면  
    첫 번째 문장 부분은 segment_id 0,  
    두 번째 문장 부분은 segment_id 1의 값을 가짐  
  3. Input Masks : padding으로 처리한 부분은 0의 값을 가지도록 마스킹  
    Self-attention을 계산할 때 Mask = 0인 부분에는 가중치를 주지 않음.  
    
BERT는 사전학습에서 [CLS] 문장 1 [SEP] 문장 2 [SEP]의 형식으로 raw corpus에 대한 Self-supervised learning을 진행했습니다.  
그러므로 다운스트림 태스크에 대해 fine-tuning할 때에도 위의 형식과 동일하게 인풋을 넣어주는 것입니다.  
만약 단일 텍스트 분류와 같이 "문장 2"부분이 없다면,  [CLS] 문장 1 [SEP] 까지 input으로 주고, segment_id는 모두 0을 부여하면 됩니다.  
이렇게 인풋을 받게 되면 BERT 내부적으로 input_id와 segment_id는 임베딩을 통해 변환됩니다.  



또한, Transformer 모델이 사용하는 Position Embedding도 계산되어 이 세 가지가 합쳐진 인풋이 Transformer 인풋으로 들어갑니다.
 
## 2. BERT 코드
  ```
  SEQ_LEN = 10
 
  ## 인풋 텐서 세 가지를 정의합니다.
  tmp_input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=tf.int32,  name="input_word_ids")
  tmp_input_mask = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=tf.int32, name="input_mask")
  tmp_segment_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=tf.int32, name="segment_ids")

  ## bert_layer을 거쳐 나온 아웃풋을 tmp_pooled_output과 tmp_seq_output에 저장하고,
  ## 각 아웃풋의 shape를 프린트해보겠습니다.
  tmp_pooled_output, tmp_sequence_output = bert_layer([tmp_input_ids, tmp_input_mask, tmp_segment_ids])

  print("** tmp_pooled_output   :", tmp_pooled_output.shape)
  print("** tmp_sequence_output :", tmp_sequence_output.shape)
  
  👉 pooled_output은 BATCH_SIZE * 768 차원의 텐서입니다. 각 입력에 대해 입력 텍스트 전체의 의미를 담은 768 차원의 벡터입니다.     
  👉 sequence_output은 BATCH_SIZE * SEQ_LEN * 768 차원의 텐서입니다. 각 토큰에 대한 768차원의 의미 벡터를 리턴한 것으로 볼 수 있습니다.   
  ```
  
 ```
 의도분류 만들기 
 ## input layer
  input_word_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name="input_word_ids")
  input_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name="input_mask")
  segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name="segment_ids")

  ## bert layer -> 위에서 불러옴
  # bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)

  ## 1. dropout p = 0.2의 드롭아웃 레이어를 정의하세요
  dropout_layer = tf.keras.layers.Dropout(0.2)
  ## 2. 176가지 의도 카테고리에 대한 확률을 리턴할 수 있는 Dense 레이어를 정의하세요
  fc_layer = tf.keras.layers.Dense(176, activation = tf.nn.softmax)
  
  
  from tensorflow.keras import Model
  ## bert_layer을 통해 pooled_output, sequence_output 받아오기
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])

  """ Your Code Here """
  ## 1. pooled_output에 dropout_layer 적용하기
  dropout_output = dropout_layer(pooled_output)  # [★CODE 1★]
  ## 2. dropout_output에 fc_layer 적용하기
  logits = fc_layer(dropout_output)  # [★CODE 2★]

  ## 3. input, output을 연결하여 model로 만들기
  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=logits)  # [★CODE 3★]
  
 ```



