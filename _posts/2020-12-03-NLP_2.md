---
layout: post
title:  "NLP 3ì¼ì°¨ : BERT"
date:   2020-12-03
author: ë‹¨ìš°ì•„ë²”
categories: "ì–¸ì–´ì§€ëŠ¥"
tags:	
cover:  "/assets/instacode.png"
---

# 2-1 BERT ë„Œ ëˆ„êµ¬ë‹ˆ
## 1. BERTë€
 BERT : Bidirectional Encoder Representations from Transformers  
 - Modelì˜ íŠ¹ì§•
  - Bi-directional
  - Transformer êµ¬ì¡° ì´ìš©  
 - ì‚¬ì „í•™ìŠµ ê³¼ì œ ìˆ˜í–‰ í›„ Fine-tuning
  - Masked Language Model
  - Next Sentence Prediction  
  
 BERT ì‚¬ì „í•™ìŠµ ê³¼ì œ 1 : Masked Language Model
  - ê°€ë ¤ì§„ ë‹¨ì–´ë¥¼ ë§ì¶”ëŠ” ê³¼ì œë¥¼ í•´ê²°í•¨ìœ¼ë¡œì¨ ì£¼ë³€ ë§¥ë½ì— ë”°ë¥¸ ì˜ë¯¸ í•™ìŠµ
  - ìì—°ì–´ í† í°ì˜ ì˜ë¯¸ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•¨
  - Transformer Encoder êµ¬ì¡°ë¥¼ ì´ìš©í•¨
  
 BERT ì‚¬ì „í•™ìŠµ ê³¼ì œ 2 : Next Sentence Prediction
  - ì œì‹œëœ ë‘ ë¬¸ì¥ì´ ì´ì–´ì§„ ë¬¸ì¥ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ë§ì¶”ëŠ” ê³¼ì œë¥¼ ìˆ˜í–‰
  - ë¬¸ë§¥ì˜ ì¼ê´€ì„±ì„ í•™ìŠµí•¨
 
 BERT Fine-tuning : ì£¼ì–´ì§„ ê³¼ì œ ìœ í˜•ì— ë”°ë¼ì„œ ë§ˆì§€ë§‰ output layerë§Œ ë³€ê²½í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ fine-tuningí•¨  
 
 BERT ìš”ì•½ : Self-supervised learning : ë°©ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¡œ ìˆ˜í–‰í•œ ì‚¬ì „í•™ìŠµ ê³¼ì œì˜ í˜
  - ì‚¬ì „ í•™ìŠµ ê³¼ì œ
    - ì—„ì²­ë‚œ ìì›ì„ ì‚¬ìš©í•˜ì—¬ 4ì¼ ë‚´ë‚´ í•™ìŠµí•¨
  - Fine-tuning
    - íƒœìŠ¤í¬ì— ë”°ë¼ 1~4 epochë§Œ ì¶”ê°€ ìˆ˜í–‰
 
---
 
# 2-2 BERT í† í¬ë‚˜ì´ì§•
## 1. BPE, Wordpiece Tokenization
 UNKê°€ ë§ì´ ìƒê¸°ì§€ ì•Šìœ¼ë©´ì„œë„ í•„ìš”í•œ ê²½ìš° ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” í† í° ë‹¨ìœ„ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•, Byte Pair Encodingê³¼ WordPiece Tokenization  
 
### Byte Pair Encoding (BPE)  
 BPEëŠ” __ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë¬¸ìì˜ ì¡°í•©__ ìœ¼ë¡œ ë¬¸ì¥ì„ íŒŒì‹±í•˜ëŠ” ë°©ë²•  
### WordPiece Tokeniztion  
 BPEì™€ ê±°ì˜ ë¹„ìŠ·í•˜ì§€ë§Œ, ë¹ˆë„ìˆ˜ ëŒ€ì‹  ë‹¨ì–´ì˜ likelihoodê°€ ë†’ì€ ì¡°í•©ì„ ì°¾ì•„ë‚¸ë‹¤ëŠ” ê²ƒë§Œ ì°¨ì´ê°€ ìˆìŒ  


## 2. Google Multi-lingual BERT
 êµ¬ê¸€ì—ì„œ í•œêµ­ì–´ë¥¼ í¬í•¨í•œ 102êµ­ì–´ë¥¼ ì§€ì›í•˜ëŠ” Multi-lingual BERT ëª¨ë¸ì„ ê³µê°œí•¨
 
 
---
 
# 3-1 BERT ì¸í’‹ë“¤
## 1. BERT ì¸í’‹
BERTëŠ” input_ids, segment_ids, input_masksë¼ëŠ” 3ê°€ì§€ input layerë¥¼ ëŒ€ì…í•¨  
  1. input_ids : ì¸í’‹ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ì‚¬ì „ì„ ì´ìš©í•´ idë¡œ ë³€í™˜í•œ ë¦¬ìŠ¤íŠ¸  
    í…ìŠ¤íŠ¸ëŠ” ìŠ¤í˜ì…œ í† í° [CLS] ì— í•´ë‹¹í•˜ëŠ” idë¡œ ì‹œì‘í•¨  
    ì†ì„±ì´ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì‚¬ì´ í˜¹ì€ ì¸í’‹ì´ ëë‚¬ì„ ë•ŒëŠ” [SEP] í† í° ì‚¬ìš©  
  2. Segment_ids : í…ìŠ¤íŠ¸ ë¶„ì„ì˜ ë‹¨ìœ„ë¥¼ 0 í˜¹ì€ 1ë¡œ ë‚˜íƒ€ë‚´ì£¼ê¸° ìœ„í•œ id  
    ì˜ˆë¥¼ ë“¤ì–´ ë‘ ê°œì˜ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” íƒœìŠ¤í¬ë¼ë©´  
    ì²« ë²ˆì§¸ ë¬¸ì¥ ë¶€ë¶„ì€ segment_id 0,  
    ë‘ ë²ˆì§¸ ë¬¸ì¥ ë¶€ë¶„ì€ segment_id 1ì˜ ê°’ì„ ê°€ì§  
  3. Input Masks : paddingìœ¼ë¡œ ì²˜ë¦¬í•œ ë¶€ë¶„ì€ 0ì˜ ê°’ì„ ê°€ì§€ë„ë¡ ë§ˆìŠ¤í‚¹  
    Self-attentionì„ ê³„ì‚°í•  ë•Œ Mask = 0ì¸ ë¶€ë¶„ì—ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì§€ ì•ŠìŒ.  
    
BERTëŠ” ì‚¬ì „í•™ìŠµì—ì„œ [CLS] ë¬¸ì¥ 1 [SEP] ë¬¸ì¥ 2 [SEP]ì˜ í˜•ì‹ìœ¼ë¡œ raw corpusì— ëŒ€í•œ Self-supervised learningì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.  
ê·¸ëŸ¬ë¯€ë¡œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì— ëŒ€í•´ fine-tuningí•  ë•Œì—ë„ ìœ„ì˜ í˜•ì‹ê³¼ ë™ì¼í•˜ê²Œ ì¸í’‹ì„ ë„£ì–´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.  
ë§Œì•½ ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜ì™€ ê°™ì´ "ë¬¸ì¥ 2"ë¶€ë¶„ì´ ì—†ë‹¤ë©´,  [CLS] ë¬¸ì¥ 1 [SEP] ê¹Œì§€ inputìœ¼ë¡œ ì£¼ê³ , segment_idëŠ” ëª¨ë‘ 0ì„ ë¶€ì—¬í•˜ë©´ ë©ë‹ˆë‹¤.  
ì´ë ‡ê²Œ ì¸í’‹ì„ ë°›ê²Œ ë˜ë©´ BERT ë‚´ë¶€ì ìœ¼ë¡œ input_idì™€ segment_idëŠ” ì„ë² ë”©ì„ í†µí•´ ë³€í™˜ë©ë‹ˆë‹¤.  



ë˜í•œ, Transformer ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” Position Embeddingë„ ê³„ì‚°ë˜ì–´ ì´ ì„¸ ê°€ì§€ê°€ í•©ì³ì§„ ì¸í’‹ì´ Transformer ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.
 
## 2. BERT ì½”ë“œ
  ```
  SEQ_LEN = 10
 
  ## ì¸í’‹ í…ì„œ ì„¸ ê°€ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
  tmp_input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=tf.int32,  name="input_word_ids")
  tmp_input_mask = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=tf.int32, name="input_mask")
  tmp_segment_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=tf.int32, name="segment_ids")

  ## bert_layerì„ ê±°ì³ ë‚˜ì˜¨ ì•„ì›ƒí’‹ì„ tmp_pooled_outputê³¼ tmp_seq_outputì— ì €ì¥í•˜ê³ ,
  ## ê° ì•„ì›ƒí’‹ì˜ shapeë¥¼ í”„ë¦°íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤.
  tmp_pooled_output, tmp_sequence_output = bert_layer([tmp_input_ids, tmp_input_mask, tmp_segment_ids])

  print("** tmp_pooled_output   :", tmp_pooled_output.shape)
  print("** tmp_sequence_output :", tmp_sequence_output.shape)
  
  ğŸ‘‰ pooled_outputì€ BATCH_SIZE * 768 ì°¨ì›ì˜ í…ì„œì…ë‹ˆë‹¤. ê° ì…ë ¥ì— ëŒ€í•´ ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ ë‹´ì€ 768 ì°¨ì›ì˜ ë²¡í„°ì…ë‹ˆë‹¤.     
  ğŸ‘‰ sequence_outputì€ BATCH_SIZE * SEQ_LEN * 768 ì°¨ì›ì˜ í…ì„œì…ë‹ˆë‹¤. ê° í† í°ì— ëŒ€í•œ 768ì°¨ì›ì˜ ì˜ë¯¸ ë²¡í„°ë¥¼ ë¦¬í„´í•œ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   
  ```
  
 ```
 ì˜ë„ë¶„ë¥˜ ë§Œë“¤ê¸° 
 ## input layer
  input_word_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name="input_word_ids")
  input_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name="input_mask")
  segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name="segment_ids")

  ## bert layer -> ìœ„ì—ì„œ ë¶ˆëŸ¬ì˜´
  # bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)

  ## 1. dropout p = 0.2ì˜ ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì •ì˜í•˜ì„¸ìš”
  dropout_layer = tf.keras.layers.Dropout(0.2)
  ## 2. 176ê°€ì§€ ì˜ë„ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ í™•ë¥ ì„ ë¦¬í„´í•  ìˆ˜ ìˆëŠ” Dense ë ˆì´ì–´ë¥¼ ì •ì˜í•˜ì„¸ìš”
  fc_layer = tf.keras.layers.Dense(176, activation = tf.nn.softmax)
  
  
  from tensorflow.keras import Model
  ## bert_layerì„ í†µí•´ pooled_output, sequence_output ë°›ì•„ì˜¤ê¸°
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])

  """ Your Code Here """
  ## 1. pooled_outputì— dropout_layer ì ìš©í•˜ê¸°
  dropout_output = dropout_layer(pooled_output)  # [â˜…CODE 1â˜…]
  ## 2. dropout_outputì— fc_layer ì ìš©í•˜ê¸°
  logits = fc_layer(dropout_output)  # [â˜…CODE 2â˜…]

  ## 3. input, outputì„ ì—°ê²°í•˜ì—¬ modelë¡œ ë§Œë“¤ê¸°
  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=logits)  # [â˜…CODE 3â˜…]
  
 ```



