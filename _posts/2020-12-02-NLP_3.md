---
layout: post
title:  "NLP 2일차 : Transformer"
date:   2020-12-02
author: 단우아범
categories: "언어지능"
tags:	
cover:  "/assets/instacode.png"
---

# 3-1 : Transformer
## 0. Transformer는 왜 탄생했는가?
 - RNN의 Long-term dependency를 해결하기 위해 LSTM, GRU와 같은 셀에 대해 공부하였음.  
 그럼에도 불구하고, Seq2Seq에서 먼 거리에 있는 토큰 사이의 관계를 모델링 하기 위해선 attention과 같은 지름길 디자인이 추가로 필요했음  
 
 Attention을 통해 먼 거리에 있는 문맥 정보도 가지고 와서 사용할 수 있게 되었지만,  
 시퀀스 토큰을 타임스텝별로 하나씩 처리해야 하는 RNN의 특성상, 느리다는 단점은 여전히 해결되지 않음
 
 - RNN 없이
  - 입력된 시퀀스에 있는 정보를 잘 모델링하고
  - 주어진 문맥 내의 모든 정보를 고려해 자연어 토큰들의 정보를 모델링할 수 있으면서
  - 병렬처리가 가능해 속도가 빠른  
 모델은 만들 수 없는 것일까?  
 
 이러한 필요에 의해 탄생한 아키텍처가 바로 `Transformer` 아키텍처임
 
## 1. Transformer란?
  <img src = "">  
  
Transformer는 RNN이나 CNN과 없이 오로지 <attention>과  <feed forward network(=Dense layer)>만으로 이루어져 있습니다.  
기존에 RNN 모델은 타임스텝별로 들어오는 인풋을 이전 정보와 결합하여 hidden representation을 생성했습니다.  
그런데 Transformer에서는, 한 번에 문맥 내에 있는 모든 토큰 정보를 고려해 hidden representation을 생성합니다.  
  
Attention을 통해 각 토큰이 집중해야 할 문맥 정보를 점수매기고, 가져오는 방식(가중합)으로 말입니다.  
__RNN이 끊어 읽기라면, Transformer은 "속독"을 하고 있는 셈__ 이지요.

 
