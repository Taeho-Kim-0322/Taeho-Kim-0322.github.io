---
layout: post
title:  "NLP 2일차 : Transformer"
date:   2020-12-02
author: 단우아범
categories: "언어지능"
tags:	
cover:  "/assets/instacode.png"
---

# 3-1 : Transformer
## 0. Transformer는 왜 탄생했는가?
 RNN의 Long-term dependency를 해결하기 위해 LSTM, GRU와 같은 셀에 대해 공부하였음.  
 그럼에도 불구하고, Seq2Seq에서 먼 거리에 있는 토큰 사이의 관계를 모델링 하기 위해선 attention과 같은 지름길 디자인이 추가로 필요했음  
 
 Attention을 통해 먼 거리에 있는 문맥 정보도 가지고 와서 사용할 수 있게 되었지만,  
 시퀀스 토큰을 타임스텝별로 하나씩 처리해야 하는 RNN의 특성상, 느리다는 단점은 여전히 해결되지 않음
 
 RNN 없이
  - 입력된 시퀀스에 있는 정보를 잘 모델링하고
  - 주어진 문맥 내의 모든 정보를 고려해 자연어 토큰들의 정보를 모델링할 수 있으면서
  - 병렬처리가 가능해 속도가 빠른  
모델은 만들 수 없는 것일까?  
 
 이러한 필요에 의해 탄생한 아키텍처가 바로 `Transformer` 아키텍처임
 
---
 
## 1. Transformer란?
  <img src = "https://user-images.githubusercontent.com/59005950/100830387-e2d8b300-34a6-11eb-83b3-ba5719940282.png">  
  
Transformer는 RNN이나 CNN과 없이 오로지 attention과 feed forward network(=Dense layer)만으로 이루어져 있습니다.  
기존에 RNN 모델은 타임스텝별로 들어오는 인풋을 이전 정보와 결합하여 hidden representation을 생성했습니다.  
그런데 Transformer에서는, 한 번에 문맥 내에 있는 모든 토큰 정보를 고려해 hidden representation을 생성합니다.  
  
Attention을 통해 각 토큰이 집중해야 할 문맥 정보를 점수매기고, 가져오는 방식(가중합)으로 말입니다.  
__RNN이 끊어 읽기라면, Transformer은 "속독"을 하고 있는 셈__ 이지요.

  핵심은, scaled dot-product attention  
  <img src = "https://user-images.githubusercontent.com/59005950/100830394-e409e000-34a6-11eb-8fe0-d1795ad4288b.png">  
  
  Transformer 구조는 위의 그림과 같음  
기본적으로 번역 태스크를 수행하기 위해 제안된 모델이기 때문에 인코더-디코더 구조를 가집니다.  
다만, 인코더와 디코더에 RNN을 사용하는 대신 Transformer 구조를 사용합니다.  
인코더 부분의 Transformer 블록(회색 상자)을 보면 인풋을 입력받아  
  1. Multi-Head Attention
  2. Add & Norm
  3. Feed Forward
  4. Add & Norm  
연산을 하는 것을 볼 수 있습니다.  

Add & Norm은 residual connection을 만들고, 레이어 정규화를 통해 모델이 정보를 잃어버리지 않도록 하기 위해 있는 레이어입니다.  
Feed Forward는 우리가 아는 Dense Layer로, Multi-head attention에서 나온 hidden state의 정보를 가공하는 레이어입니다.  
그런데 Multi-head attention이라는 것은 무엇일까요?  
Zoom-In 그림을 보니, Multi-head attention은 여러 개의 Scaled dot-product attention으로 이루어져있는 것을 확인할 수 있습니다.  
이 Scaled dot-product attention이 바로 Transformer 구조의 핵심입니다.  

  <Scaled dot-product attention>
  <img src = "https://user-images.githubusercontent.com/59005950/100830396-e4a27680-34a6-11eb-9cc3-496871f37931.png">
  
  Scaled dot-product attention은  
    1. dot-product로 attention 점수를 계산하고  
    2. 그 점수를 scaling한다  
  는 의미에서 붙여진 이름임  
  
---
  
# 3-2 : Self Attention
## 1. Self Attention
  Self Attention이란 주어진 인풋 문장에서 토큰의 의미를 모델링하는 과정을 뜻함  
    - Step 1 : Attention 대상이 되는 토큰들을 key, value, attention하는 토큰을 query로 변환  
    - Step 2 : Query에 대해 각 key들과의 내적을 통해 attention 가중치 계산  
    - Step 3 : 가중치를 이용해 value를 가중합하여 query의 representation을 업데이트함
  ....  
  
## 2. Self Attention이 중요한 이유
  Self attention은 인풋 시퀀스 전체에 대해 attention을 계싼해 각 토큰의 representation을 만들어가는 과정으로, 업데이트된 representation은 문맥 정보를 가지고 있음  
  게다가 이러한 과정은 타임스텝에 따른 지연 없이 이루어지며 인풋으로 들어온 모든 문맥의 정보를 forgetting 없이 반영할 수 있음  
  RNN의 단점이었던 느리다와 long-term dependency의 문제가 해결된 것임
  
  
## 3. Multi-head attention
  
---
  
# 3-3 : Positional Encoding
  <img src = "https://user-images.githubusercontent.com/59005950/100831730-b5413900-34a9-11eb-9fde-3fc5c4e42ef2.png">  
  단어의 순서 정보를 고려하기 위해 Positional Encoding을 추가로 더해줌  
  
  <img src = "https://user-images.githubusercontent.com/59005950/100831732-b70afc80-34a9-11eb-819c-c474302aa602.png">  
  Sin & Cos 함수를 사용해 인위적으로 시그널을 만드는 것
  
---
  
# 3-4 : 요약
<img src = "https://user-images.githubusercontent.com/59005950/100831889-ffc2b580-34a9-11eb-8381-b919aa8523c0.png">  
Transformer 블록은 인풋을 받아 토큰 인베딩을 진행하고, 위치 정보를 더해준 후 Multi-head attention을 계산함.  
Multi-head attention에서는 scaled dot-product attention을 통해 인풋에 있는 모든 문맥 정보를 사용해 토큰의 representation을 만듬.  
이렇게 가공된 정보는 Fully Connected Network를 통해 한 번 더 정보를 변환하는 과정을 거침.  
이렇게 생긴 Transformer 블록을 여러 층 쌓아서 정보를 더 깊게 모딜링함  

## Transformer의 특징
  1. RNN을 통해 각 타임스텝의 hidden state이 계산되기를 기다리지 않아도 된다.
  2. 즉, 문장에 있는 모든 단어의 representation들을 병렬적으로 한번에 만들 수 있다 →  빠르다!
  3. 기존의 RNN, CNN 방식을 과감히 포기하고 Attention만을 사용해 문맥을 모델링한다는 점에서 획기적인 알고리즘이다.
  4. 병렬화와 학습 시간 단축에 기여했고, Long-Term Dependency를 해결한 아키텍쳐다.
  5. 각종 기계번역 대회에서 세계 최고의 기록 보유 ! ! (WMT 2014 등)
  6. 이후 BERT, GPT 등 최신 언어 AI 알고리즘은 모두 이 Transformer 아키텍처를 기반으로 삼고 있다.

  
  
  
