---
layout: post
title:  "NLP 2일차 : Transformer"
date:   2020-12-02
author: 단우아범
categories: "언어지능"
tags:	
cover:  "/assets/instacode.png"
---

# 3-1 : Transformer
## 0. Transformer는 왜 탄생했는가?
 - RNN의 Long-term dependency를 해결하기 위해 LSTM, GRU와 같은 셀에 대해 공부하였음.  
 그럼에도 불구하고, Seq2Seq에서 먼 거리에 있는 토큰 사이의 관계를 모델링 하기 위해선 attention과 같은 지름길 디자인이 추가로 필요했음  
 
 Attention을 통해 먼 거리에 있는 문맥 정보도 가지고 와서 사용할 수 있게 되었지만,  
 시퀀스 토큰을 타임스텝별로 하나씩 처리해야 하는 RNN의 특성상, 느리다는 단점은 여전히 해결되지 않음
 
 - RNN 없이
  - 입력된 시퀀스에 있는 정보를 잘 모델링하고
  - 주어진 문맥 내의 모든 정보를 고려해 자연어 토큰들의 정보를 모델링할 수 있으면서
  - 병렬처리가 가능해 속도가 빠른  
 모델은 만들 수 없는 것일까?  
 
 이러한 필요에 의해 탄생한 아키텍처가 바로 `Transformer` 아키텍처임
 
## 1. Transformer란?
  <img src = "https://user-images.githubusercontent.com/59005950/100830387-e2d8b300-34a6-11eb-83b3-ba5719940282.png">  
  
Transformer는 RNN이나 CNN과 없이 오로지 <attention>과  <feed forward network(=Dense layer)>만으로 이루어져 있습니다.  
기존에 RNN 모델은 타임스텝별로 들어오는 인풋을 이전 정보와 결합하여 hidden representation을 생성했습니다.  
그런데 Transformer에서는, 한 번에 문맥 내에 있는 모든 토큰 정보를 고려해 hidden representation을 생성합니다.  
  
Attention을 통해 각 토큰이 집중해야 할 문맥 정보를 점수매기고, 가져오는 방식(가중합)으로 말입니다.  
__RNN이 끊어 읽기라면, Transformer은 "속독"을 하고 있는 셈__ 이지요.

  - 핵심은, scaled dot-product attention  
  <img src = "https://user-images.githubusercontent.com/59005950/100830394-e409e000-34a6-11eb-8fe0-d1795ad4288b.png">  
  
  Transformer 구조는 위의 그림과 같음  
기본적으로 번역 태스크를 수행하기 위해 제안된 모델이기 때문에 인코더-디코더 구조를 가집니다.  
다만, 인코더와 디코더에 RNN을 사용하는 대신 Transformer 구조를 사용합니다.  
인코더 부분의 Transformer 블록(회색 상자)을 보면 인풋을 입력받아  
  1. Multi-Head Attention
  2. Add & Norm
  3. Feed Forward
  4. Add & Norm  
연산을 하는 것을 볼 수 있습니다.  

Add & Norm은 residual connection을 만들고, 레이어 정규화를 통해 모델이 정보를 잃어버리지 않도록 하기 위해 있는 레이어입니다.  
Feed Forward는 우리가 아는 Dense Layer로, Multi-head attention에서 나온 hidden state의 정보를 가공하는 레이어입니다.  
그런데 Multi-head attention이라는 것은 무엇일까요?  
Zoom-In 그림을 보니, Multi-head attention은 여러 개의 Scaled dot-product attention으로 이루어져있는 것을 확인할 수 있습니다.  
이 Scaled dot-product attention이 바로 Transformer 구조의 핵심입니다.  

  <Scaled dot-product attention>
  <img src = "https://user-images.githubusercontent.com/59005950/100830396-e4a27680-34a6-11eb-9cc3-496871f37931.png">
  
  - Scaled dot-product attention은
    1. dot-product로 attention 점수를 계산하고
    2. 그 점수를 scaling한다
  는 의미에서 붙여진 이름임  
  
  
