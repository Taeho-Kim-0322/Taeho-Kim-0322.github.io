---
layout: post
title: "분석_프로세스"
date: 2021-01-07
author: 단우아범
categories: "머신러닝"
tags:	
cover: "/assets/instacode.png"
---

캐글 데이터 분석 강의를 기초로 해서, 대략적인 분석 프로세스에 대해 정리해보자.  


# 데이터 확인  
데이터를 확인하는 단계가 가장 우선이다. 대충 보는 것이 아닌 자세하게 봐야 한다. 데이터 분석 노트를 만들어 놓은 것을 활용한다.  
활용가능한 테이블은 무엇이 있는지, 주요 칼럼은 어떤지, 데이터 타입은, distinct 값 등에 대한 품질검사를 실시하여 정리해 놓는다.  
  
  
# 초기 모델 구현  
변수들을 어떻게 처리할 지, 파생변수는 무엇을 만들지 등 여러 고민을 하게 되는데, 이보다는 우선 간단한 모델로 초기 결과값을 내는 것을 추천한다.  

# Feature Engineering  
알고리즘이 보다 효율적으로 예측할 수 있도록 데이터를 가공  
`전통적인 FE : 스케일링, 변환(로그, PCA), 인코딩(레이블, 원핫), 결측치/이상치 치환, Skew데이터 보정(오버/언더 샘플링) 등  
업무적 이해에 기반한 FE : 중요 피처들을 결합/재가공해 새로운 파생변수 생성, 업무와 연관성이 높은 새로운 피처 재생산`  
모델 고도화, 결과값 고도화를 위해 주요 feature 값들의 분포를 확인하여 Target에 따라 차이가 큰 feature들을 주로 하여 가공한다.  
  ## 주요 Feature 값들의 EDA  
  - 기초 분포상태 확인  
    - app_train.info()  
    - app_train.isnull().sum()  
    - 종속변수 값 : app_train[종속].value_counts()  
    - 
  - 시각화  
    - 연속형 숫자 feature : Histogram 시각화 → violinplot과 distplot으로 target별로 분포 비교 시각화  
      ```
      def show_hist_by_target(df, columns):
          cond_1 = (df['TARGET'] == 1)
          cond_0 = (df['TARGET'] == 0)

          for column in columns:
              fig, axs = plt.subplot(figsize=(12,4), nrows=1, ncols=2, squeeze=False)
              sns.violinplot(x='TARGET', y=column, data=df, ax=axs[0][0])
              sns.distplot(df[cond_1][column], label='1', color='red', ax=axs[0][1])
              sns.distplot(df[cond_0][column], label='0', color='red', ax=axs[0][1])
              
        그 후, 
        이런 식으로 전체 확인하여 차이가 큰 컬럼들을 추려낸다.
        
        columns = ['AMT_INCOME_TOTAL','AMT_CREDIT', 'AMT_ANNUITY']

        show_hist_by_target(app_train, columns)
      ```
    - 카테고리 feature : seaborn의 countplot() 또는 catplot()을 이용하여 TARGET 유형에 따라 Count 비교
      ```
      #countplot
      object_columns = app_train.dtypes[app_train.dtypes=='object'].index.tolist()
      def show_count_by_target(df, columns):
        cond_1 = (df['TARGET'] == 1)
        cond_0 = (df['TARGET'] == 0)

        for column in columns:
            print('column name:', column)
            fig, axs = plt.subplots(figsize=(12,4), nrows=1, ncols=2, squeeze=False)
            sns.countplot(df[cond_0][column], ax=axs[0][0])
            sns.countplot(df[cond_1][column], ax=axs[0][1])

      show_count_by_target(app_train, object_columns)
      
      #catplot
      def show_category_by_target(df, columns):
      for column in columns:
        print('column name:', column)
        chart = sns.catplot(x=column, col= 'TARGET', data=df, kind='count')
        chart.set_xticklabels(rotation=65)

      show_category_by_target(app_train, object_columns)
      ```
  - 확인한 주요 feature들의 상관도 및 heatmap 시각화  
    ```
    corr_columns = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE',
               'DAYS_EMPLOYED','DAYS_ID_PUBLISH', 'DAYS_REGISTRATION', 'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL', 'TARGET']

    col_corr = app_train[corr_columns].corr()
    col_corr

    plt.figure(figsize=(8,8))
    sns.heatmap(col_corr, annot=True)
    ```
  - 이상치 데이터 확인 및 값 변경  
    ```
    app_train['DAYS_EMPLOYED'] = app_train['DAYS_EMPLOYED'].replace(365243, np.nan)
    app_train['DAYS_EMPLOYED'].value_counts()
    ```
  - 주요 변수를 활용한 새로운 파생변수 생성  
    ```
    #표준편차, 평균 등
    apps['APPS_EXT_SOURCE_MEAN'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)
    apps['APPS_EXT_SOURCE_STD'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)
    apps['APPS_EXT_SOURCE_STD'] = apps['APPS_EXT_SOURCE_STD'].fillna(apps['APPS_EXT_SOURCE_STD'].mean())
    
    #비율 등을 구함
    apps['APPS_ANNUITY_CREDIT_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_CREDIT']
    apps['APPS_GOODS_CREDIT_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_CREDIT']
    apps['APPS_CREDIT_GOODS_DIFF'] = apps['AMT_CREDIT'] - apps['AMT_GOODS_PRICE']
    ```  

# 고도화 모델 구현  

