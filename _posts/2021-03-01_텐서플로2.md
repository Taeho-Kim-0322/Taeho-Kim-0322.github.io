---
layout: post
title: "텐서플로_DenseLayer"
date: 2021-03-1
author: 단우아범
categories: "딥러닝"
tags:	
cover: "/assets/instacode.png"
---

기본 Dense Layer는 무엇인가!  

# 1. Dense Layer  
Dense Layer는 딥러닝에서 가장 기본이 되는 Layer임  
Neuron (Node) 내부는 W (weight) * X (입력값) 으로 이루어져 있음. (층마다 + bias도 붙습니다.)  
Dense Layer는 Fully Connected Layer(FC)라고도 불리웁니다.  

## Input Layer에서는 input_shape을 설정해 줘야 함  
input에서는 데이터가 어떤 모양으로 들어오는 지 설정을 해 주어야 함  
```
model = Sequential([
  Dense(3(노드개수), input_shpae = [1]),
  ## input shape를 넣을 때는 리스트도 되고, 튜플도 가능함
  ## input_shape = [1] or (1,)   (1)(X)
  
  Dense(4),
  Dense(4),
  Dense(1)
])
```


## 1-1. 실습1  
모든 딥러닝 프로세스틑 import - 전처리 - 모델링 - 컴파일 - 학습(fit) - 예측  
위에서 모델링을 했으니, 컴파일을 하고 학습하고 예측함  
```
model.compile(optimizer='sgd', loss='mse')
#optimizer는 보통 모멘텀이 들어간 adam을 사용함  

model.fit(xs, ys, epochs=1200, verbose=0)
#verbose = 1 → epoch의 로그가 다 찍힘

# output
# 16.000046
model.predict([10.0])
```

# 2. 과대적합과 과소적합  
(머신러닝 개념)  
- Bias : f 모델을 설정함으로써 필연적으로 생길 수 밖에 없는 오차 (e.g. 선형식을 선택함으로써 생기는 오류)  
- Variance : Training Data가 변경될 때마가 추정한 f가 변동되는 오차 (e.g. 데이터가 변경됨에 따라 회귀계수의 변동) ▶ 데이터 의존도  

- Bias ↔ Variance Trade-off  

- flexible하다(유연하다) = overfitting되었다  
  - overfitting 방지를 위해 Generalization, Early Stopping 등등  
  - [Generalization]  
    - L1 norm(계단거리) : Lasso  
    - L2 norm(직선거리) : Ridge  
    - 계수들을 '0'으로 보내는 상수항을 추가하여 Shrinkage 효과를 넣음  
    - Bias를 조금 올리는 대신 Variance를 크게 감소시켜 overfitting을 막음  
    - 데이터 n보다 변수 p가 월등히 많을 때, p>n일 때 효과적임  
    - Lasso ↔ Ridge  



