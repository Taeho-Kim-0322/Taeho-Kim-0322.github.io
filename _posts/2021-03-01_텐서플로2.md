---
layout: post
title: "텐서플로_DenseLayer"
date: 2021-03-1
author: 단우아범
categories: "딥러닝"
tags:	
cover: "/assets/instacode.png"
---

기본 Dense Layer는 무엇인가!  

# 1. Dense Layer  
Dense Layer는 딥러닝에서 가장 기본이 되는 Layer임  
Neuron (Node) 내부는 W (weight) * X (입력값) 으로 이루어져 있음. (층마다 + bias도 붙습니다.)  
Dense Layer는 Fully Connected Layer(FC)라고도 불리웁니다.  

## Input Layer에서는 input_shape을 설정해 줘야 함  
input에서는 데이터가 어떤 모양으로 들어오는 지 설정을 해 주어야 함  
```
model = Sequential([
  Dense(3(노드개수), input_shpae = [1]),
  ## input shape를 넣을 때는 리스트도 되고, 튜플도 가능함
  ## input_shape = [1] or (1,)   (1)(X)
  
  Dense(4),
  Dense(4),
  Dense(1)
])
```


## 1-1. 실습1  
모든 딥러닝 프로세스틑 import - 전처리 - 모델링 - 컴파일 - 학습(fit) - 예측  
위에서 모델링을 했으니, 컴파일을 하고 학습하고 예측함  
```
model.compile(optimizer='sgd', loss='mse')
#optimizer는 보통 모멘텀이 들어간 adam을 사용함  

model.fit(xs, ys, epochs=1200, verbose=0)
#verbose = 1 → epoch의 로그가 다 찍힘

# output
# 16.000046
model.predict([10.0])
```

# 2. 과대적합과 과소적합  
(머신러닝 개념)  
- Bias : f 모델을 설정함으로써 필연적으로 생길 수 밖에 없는 오차 (e.g. 선형식을 선택함으로써 생기는 오류)  
- Variance : Training Data가 변경될 때마가 추정한 f가 변동되는 오차 (e.g. 데이터가 변경됨에 따라 회귀계수의 변동) ▶ 데이터 의존도  
- Bias ↔ Variance Trade-off  
- flexible하다(유연하다) = overfitting되었다  
  - overfitting 방지를 위해 Generalization, Early Stopping 등등  
  - [Generalization]  
    - L1 norm(계단거리) : Lasso  
    - L2 norm(직선거리) : Ridge  
    - 계수들을 '0'으로 보내는 상수항을 추가하여 Shrinkage 효과를 넣음  
    - Bias를 조금 올리는 대신 Variance를 크게 감소시켜 overfitting을 막음  
    - 데이터 n보다 변수 p가 월등히 많을 때, p>n일 때 효과적임  
    - Lasso ↔ Ridge  
- Training셋과 Validation 셋을 구분하여 과적합/과소적합을 방지함  
- ModelCheckpoint를 활용해서 사용함  
```
checkpoint_path = "저장경로"
checkpoint = ModelCheckpoint(filepath = checkpoint_path,
                           save_weights_only = True,
                           save_best_only = True,
                           monitor="val_loss",
                           verbose=1)

history = model.fit(x_train,y_train,
                    validation_data = (x_valid, y_valid),
                    epochs=20,
                    callbacks=[checkpoint],
                    )
                    
#학습이 종되되면, 모델에 최적의 weights를 가져와야 함
model.load_weights(checkpoint_path)
```
- 

# 3. 이미지 데이터 전처리 - 정규화  
모든 이미지는 0~255의 숫자로 되어 있음  
모든 픽셀 값이 0~1사이의 값을 가지도록 정규화함  
정규화는 데이터를 보고 결정해야 함  

# 4. 원핫인코딩  
범주형 변수들의 상관성을 반영하지 않고, 독립적인 의미를 부여하기 위해 원핫인코딩을 진행함  


# 5. 활성함수(activation function)  
Desne 레이어는 기본적인 선형함수임  
선형함수만 층을 깊게 쌓게 되면, 선형X선형 = 선형관계이므로 층을 깊게 하는 의미가 줄어들기 때문임  
복잡한 문제를 풀기 어려움  
비선형 함수를 활성화 함수라고 함.  
relu, sigmoid, softmax 등이 있음  
그래서, 선형 - 비선형 - 선형 - 비선형 등으로 레이어를 쌓음  
```
tf.keras.layers.Dense(512),
tf.keras.layers.ReLU(),
tf.keras.layers.Dense(256),
tf.keras.layers.ReLU(),
tf.keras.layers.Dense(10),
tf.keras.layers.Softmax()

▶ 

tf.keras.layers.Dense(512, activation='relu'),
tf.keras.layers.Dense(256, activation='relu'),
tf.keras.layers.Dense(10, activation='softmax')
```
- 마지막 output 출력층의 활성함수는 sigmoid or softmax를 쓰는데,  
- 마지막 활성함수가 달라지면, compile에서 loss 함수가 달라짐  
- Dense(1, activation = 'sigmoid') → loss = 'binary_crossentropy'  
- Dense(2이상, activation='softmax') → loss = 'categorical_crossentropy'(원핫인코딩시), or 'sparse_categorical_crossentropy' (원핫인코딩 사용 X시)  
- sigmoid와 softmax 차이  
- sigmoid는 Dense가 1, 0.5이상이면 1, 0.5 미만이면 0 으로 분류 / softmax는 Dense가 2 이상, 카테고리별로 분류확률을 비교해서 분류함  

# 6. Flatten이란?  
- 고차원을 1D로 변환하여 **Dense Layer에 전달**해 주기 위하여 사용함  
- 28 X 28 의 **2D**로 되어 있는 이미지를 784로 **1D로 펼쳐 주는 작업** 임  
```
model = Sequential([
    # Flatten으로 shape 펼치기
    Flatten(input_shape=(28, 28)),
    # Dense Layer
    Dense(1024, activation='relu'),
    Dense(512, activation='relu'),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    # Classification을 위한 Softmax 
    Dense(10, activation='softmax'),
])
```
- 

