---
layout: post
title:  "NLP 1일차 : NLU FLOW"
date:   2020-12-01 
author: 단우아범
categories: "언어지능"
tags:	
cover:  "/assets/instacode.png"
---
# Day 1 : NLU란 무엇인가? NLU 전체 FLOW 이해하기

# 2-1 : NLU FLOW
## 0. 언어AI가 꼭 거쳐야 할 4가지 단계
 <img src="https://user-images.githubusercontent.com/59005950/100683413-a8deb280-33bb-11eb-81e5-92e44430e20d.png/"> 

---

# 2-2 : 토크나이징
## 0. 토크나이징이란?
  - 모델은 자연어 문장을 일정 단위로 쪼개서 입력받고 처리하는데, 분석의 단위가 되는 어절, 형태소, 음절, 자소 등을 지칭하는 단어가 토큰(token)임
  - 문장을 토큰 단위로 쪼개는 것을 토크나이징이라고 함
  - <img src="https://user-images.githubusercontent.com/59005950/100684367-8c437a00-33bd-11eb-91bf-dbaa852743be.png/">  
  - 토큰은 자연어 분석의 단위인만큼, 일정한 의미를 가진 단위를 선택하는 것이 ㅇ리반적
  - 영어의 경우 띄어쓰기 단위로 단어 단위를 불리할 수 있기 때문에, 어절 단위의 분석이 주로 이루어짐
  - 그러나, 복합어 특성을 가지는 한국어는 어절보다 작은 의미단위인 형태소를 주로 사용함
 
## 1. 한국어 형태소 분석의 특징
  - 음절 단위 분석을 시행하지 않는 이상, __형태소 분석은 한국어 형태소 분석__ 의 기본이라고 생각할 수 있음
  - 대표적인 형태소분석기로는
    - MeCab : 처리 속도가 매우 빠르고 분석 품질도 상위권이나, 환경에 따라 설치가 까다로움
    - Konlpy : 다양한 형태소분석기(Kkma, komoran, okt ...)를 모아놓은 파이썬 패키지로, 설치가 용이하나 품질이 떨어지는 경우가 있음
    - khaiii : 카카오에서 만든 딥러닝 기반의 형태소 분석기
  - 환경이 맞는다면 MeCab 사용을 추천함
  - 형태소 분석기에 따라 분석 결과가 크게 차이가 나고, 품사 tag에도 다르기 때문에, 한 번 선택한 형태소 분석기는 학습~추론에 있어 바꾸기 어려움
  - 적합한 형태소 분석기를 잘 선택하는 것이 NLU 분석의 중요한 시작점
  <img src="https://user-images.githubusercontent.com/59005950/100684368-8d74a700-33bd-11eb-9a64-751438932366.png/">  
 
---

# 2-3 : 인코딩
## 0. 인코딩이란?
  <img src="https://user-images.githubusercontent.com/59005950/100685016-d711c180-33be-11eb-8d46-9b1c30f8ea9e.png/">  
  - 자연어로 되어 있는 토큰을 모델에게 인식시키기 위해, 자연어에 매핑되는 정수로 바꿔주는 작업
  - 인코딩은 토큰을 `단어사전`을 이용해 one-hot-encoding으로 바꾸는 과정을 의미함
  - 단어사전이란, <토큰을 겹치지 않는 정수 인덱스로 매핑>하며 `NLU 프로세스에서 가장 신경 써야하는` 중요한 개념임
  
  <img src="https://user-images.githubusercontent.com/59005950/100685017-d7aa5800-33be-11eb-9370-6919fdcef9c0.png/">  
  - __단어사전을 학습부터 추론까지 변하지 않게 유지__ 하는 것이 중요함
 
## 1. 단어사전
  - 단어사전은 __가지고 있는 코퍼스에 대해 중복을 제거하고 빈도수 기준 Top V개__ 를 선택해 생성함
  - 예를 들어, 위키 백과, 뉴스를 크롤링해서 형태소분석을 시행한 후 상위 빈도 10만개의 단어만 사용하는 것
  
  - 단어사전의 크기 V는 임의로 설정할 수 있는데,
    - 모든 토큰을 다 담아 V가 너무 커지면 나중에 모델의 크기가 지나치게 증가하기 때문에 바람직하지 않음
    - 그렇다고 너무 적은 토큰만을 포함하면 모델이 모르는 단어가 너무 많아 퍼포먼스가 떨어짐
  - 따라서 적당한 크기의 단어사전을 만들어야 하는데, 경험적으로 5~10만개가 적당하다고 함
    
  <img src="https://user-images.githubusercontent.com/59005950/100685011-d6792b00-33be-11eb-8166-dc983b22c795.png/">  
  
  - 토큰을 선택한 후에는 분석에 필요한 __스페셜 토큰__ 을 단어사전에 포함해야 함
  - 이때 [PAD], [UNK] 토큰은 단어사전에 반드시 포함시켜야 함
    - PAD : 딥러닝 배치 처리를 위해 시퀀스 길이를 맞추는데 사용
    - UNK : 단어사전에 포함되지 않은 단어를 처리하기 위해 사용
  - 이외에 번역 과제를 수행할 경우 문장의 시작을 알리는 sos, 끝을 알리는 eos 등의 토큰도 포함해 사용할 수 있음
  
  - 스페셜 토큰은 자연적으로 발생하는 토큰이 아니라 모델 학습을 위해 사용하는 토큰이기 때문에  
  <>, []등의 기호를 포함하여 일반 토큰과 겹치지 않게 만들어야 함
  
  
---

# 2-4 : 토큰 임베딩
## 0. 토큰 임베딩
  <img src="https://user-images.githubusercontent.com/59005950/100686467-bdbe4480-33c1-11eb-8a54-c5414bd8ab0b.png/">  
  - 앞서 one-hot-encoding으로 변환된 벡터는 적당한 크기를 가지는 실수 타입의 벡터로 매핑되며, 이 과정을 토큰 임베딩이라고 부름
  
  <img src="https://user-images.githubusercontent.com/59005950/100686471-be56db00-33c1-11eb-94a4-927aa7a47fe4.png/">  
  - one-hot-vector에 임베딩 매트릭스 W를 곱하면 임베딩된 N차원의 벡터를 얻을 수 있음
  - 여기서 임베딩 매트릭스는 (단어사전크기 V x임베딩크기 N) 차원을 가지는데, 이 차원을 보면 단어사전 크기의 중요성을 다시 한번 살펴볼 수 있음
  - 단어사전 크기V가 지나치게 커지면, 이 임베딩 매트릭스 크기는 급증하기 때문
  
## 1. 좋은 임베딩이란 무엇인가?
  <img src="https://user-images.githubusercontent.com/59005950/100686490-c9117000-33c1-11eb-9710-d30066fe8eca.png/">  
  - 임베딩된 벡터들이 의미를 가진다면, 모델의 학습성능이 좋아질 것.
  - __의미 있는 벡터__ 란? 비슷한 의미를 가지는 토큰을 비슷한 공간으로 임베딩한 것
  - 예를들어, "초코"와 "초콜릿" 토큰은 비슷하게 임베딩된다면, 모델은 다양한 어휘에 대해 보다 robust한 아웃풋을 낼 수 있음  
  
  <img src="https://user-images.githubusercontent.com/59005950/100686492-ca429d00-33c1-11eb-8d08-47bfa6ecb774.png/">  
  - 사람은 단어의 의미를 어떻게 배우는가?
  - 첫 번째 예시에서 우리는 주변 문맥을 보고 빈칸에 들어갈 단어가 "깨끗"이라는 것을 유추할 수 있습니다.  
  즉 같은 문맥에서, 동일한 위치에 등장할 수 있는 단어들은 유사한 의미를 가졌을 것이다하는 것을 알 수 있습니다.  
  두 번째 예시에서 "매나니"라는 단어를 처음 보았더라도, 예시들을 보니 대충 "맨손" "맨땅에서"와 같은 의미일 것이라 추론할 수 있습니다.  
  단어의 사전적 정의를 모르더라도 등장하는 문맥을 여럿 보면 단어의 의미 특징을 뽑아낼 수 있는 것입니다.
  
  - 즉, 단어의 의미는 __해당 단어의 문맥(context)__ 이 담고 있습니다.
  
  
## 2. CBOW : Continuous Bag Of Words
  - 이렇게 단어의 의미를 문맥으로부터 유추할 수 있다는 모티브를 모델에 녹인 것이 CBOW라는 사전학습 방법임
  - CBOW는 수많은 코퍼스를 이용해서 단어 임베딩을 만드는 방법을 학습시키는 unsupervised learning 방법임
  <img src="https://user-images.githubusercontent.com/59005950/100686521-d890b900-33c1-11eb-8cb4-76a010b6b0ff.png/">
  - 주변 단어들로 단어를 유추하듯, 모델이 문서에 대해 __빈칸 맞추기__ 태스크를 수행하게 하는 것
  
  1. 데이터 준비하기
    <img src="https://user-images.githubusercontent.com/59005950/100686522-d9294f80-33c1-11eb-8684-2822645fab86.png/">
    - Step 1. 수많은 Raw Corpus를 확보함
      - 일반적인 메인에 대한 분석을 하고자 한다면, 위키백과 나무위키 뉴스 기사 등을 활용할 수 있음
      - 의학 분야 등 전문 지식에 대한 NLU 태스크를 수행시에는, 관련된 생물학 저서 등을 코퍼스로 확보해야 함
    - Step 2. 가운데 단어 앞/뒤로 w개의 토큰씩 가지고 와 X로, 가운데 단어를 Y 데이터로 만듬
      - 위 그림은 윈도우 크기가 2를 사용한 예시
      - 별도의 벨링 없이 학습 데이터를 구축하여 비지도 학습인 이유임
    
  2. 모델 아키텍처
    <img src="https://user-images.githubusercontent.com/59005950/100686516-d75f8c00-33c1-11eb-93a1-1a31eac6541c.png/">
    - Step 1. 인풋
      - 모델은 빈칸 앞/뒤에 있는 "window size"개 만큼의 토큰을 입력받음
    - Stpe 2. 인코딩
      - 입력된 토큰들은 각각 단어사전을 이용하여 V차원의 one-hot-vector로 인코딩 됨
    - Step 3. 토큰 임베딩
      - 인코딩된 벡터에 대해 임베딩 매트릭스 W를 이용해 N차원의 벡터로 임베딩 됨
    - Step 4. 문맥 벡터 만들기
      - 임베딩된 벡터들을 평균하여 인풋에 대한 하나의 벡터, 즉 '문맥 벡터'를 만듬
    - Step 5. 가운데 토큰 예측하기
      - 마지막으로 맥 벡터에 대해 가운데 단어를 예측하는 Dense Layer를 연결하여, 단어사전중 정답 토큰에 대한 점수가 가장 높아지도록 학습시킴
      - 학습 Loss로는 Negative likelihood loss를 이용하고, TF2.0 문법상으로 sparse categorical cross entropy loss를 사용함
  
  3. Skip-gram
    - 좋은 임베딩을 학습시키는 다른 방법으로는 Skip-gram이 있음
    - CBOW가 중간에 있는 단어를 맞추는 것이었다면, Skip-gram은 __단어 주변에 오는 단어를 맞추는 식__ 으로 학습하는 방법
    <img src="https://user-images.githubusercontent.com/59005950/100686520-d7f82280-33c1-11eb-9576-d39bbb959d60.png/">
  
  4. CBOW, Skip-gram으로 학습한 토큰 임베딩이 가지는 특징
    - 위 방법으로 학습된 벡터는 다음과 같은 좋은 성질을 가짐
    - 유사한 벡터의 밀집
    <img src="https://user-images.githubusercontent.com/59005950/100686573-f52cf100-33c1-11eb-8dd7-9170c6729ab0.png/">
    - 의미를 담은 수치 연산 가능  
    <img src="https://user-images.githubusercontent.com/59005950/100686573-f65e1e00-33c1-11eb-81b9-46d6dd7440ef.png/">
      - 이렇게 학습된 토큰 임베딩은 수치 연산까지 가능함
      - 자세한 결과 : <https://arxiv.org/pdf/1301.3781.pdf>
      - CBOW와 Skip-gram 방식으로 사전학습한 임베딩은 자연어 토큰이 가지는 의미를 N차원의 공간으로 잘 매핑하고, 모델 학습의 좋은 시작이 될 수 있다는 것을 보여줌
