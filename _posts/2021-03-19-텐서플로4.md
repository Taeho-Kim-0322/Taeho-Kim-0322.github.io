---
layout: post
title: "TDC_자연어처리"
date: 2021-03-19
author: 단우아범
categories: "딥러닝"
tags:	
cover: "/assets/instacode.png"
---

Text 분류는 4번 문제임  
sarcasm이라는 데이터를 활용해서 품  

sarcasm은 신문기사 링크, url, 해드라인, is_sarcastic 0 or 1 이진분류 문제임  
sarcastic은 비꼬는 이라는 뜻.  

비꼬는 신문기사인지 아닌지 이준 분류 문제임  

# 1. 언어 전처리  
 - 딥러닝 모델에 넣으려면, 문장을 수치화 시켜서 넣어 줘야 함  
 - 토큰화, 인코딩, 토큰 임베딩, 모델링 4단계를 꼭 거침  
  - 토큰화 : 토크나이저로 분해함  
  - 인코딩을 위해 단어사전을 만들어서 번호를 만듬  
  - 인코딩 된 번호를 원핫 인코딩으로 바꿔주고, 패딩으로 길이를 맞춰줌  
  - 토큰 임베딩 단계에서는 인코딩된 단어들을 의미있는 벡터로 만들어주고 차원을 줄여줌  

# 2. 모델링  
 - 단어는 원핫 인코딩으로 만들어줘야 하기때문에 차원이 많이 늘어남  
 - 차원의 저주를 없애주기 위해, Embedding Layer를 넣어준다.  
 - Embedding(vocab_size(단어사전 개수), embedding_dim(임베딩해줄 차원), input_length = max_length(단어 길이))  
 -   
 - RNN : Recurrent Neural Network (시간의 순서를 반영한 최초의 레이어임) → 자연어처리나 주가예측 등 시간의 개념이 중요한 경우  
 - RNN은 Vanishing/Exploding Gradient 문제가 발생하여 길이가 길어질수록 중요도가 떨어지는 문제가 생김  → LSTM이나 GRU 등이 생김  → Transformer → ...  
 -   
 - LSTM이란 Long Short Term Memory로, 롱텀프리퀀시 문제를 해결하기 위해 생김  
 - LSTM에서 many to one → 4번문제, many to many → 5번문제 쓸 때 사용함  
 - many to one : 문장을 통해 단일 결과를 예측할 때  
  - 리뷰를 통해 평점 예측, 리뷰의 감정 예측 등  
  - tf.keras.layers.LSTM(64)
 - many to many : 문장을 통해 시퀀스를 예측  
  - 문장을 통해 다음 문장을 예측함
  - 과거 20일의 주가 데이터로 미래 20일 주가를 예측할 때  
  - tf.keras.layers.LSTM(64, return_sequences = True)
 - LSTM 층을 겹쳐서 사용할 때에는, 겹쳐진 구간은 모두 return_sequences=True로 설정해줘야 함  
 -   
 - Bidirectional 레이어 : 양방향으로 예측함  
  - Bidirectional(LSTM()) 로 LSTM을 감싸주면 됨  
   ```
   model = Sequential([
      Embedding(vocab_size, embedding_dim, input_length=max_length),
      Bidirectional(LSTM(64, return_sequences=True)),
      Bidirectional(LSTM(64)),
      Dense(32, activation='relu'),
      Dense(16, activation='relu'),
      Dense(1, activation='sigmoid')
  ])
   ```
