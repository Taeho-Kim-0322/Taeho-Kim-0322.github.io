---
layout: post
title:  "NLP 1일차 : RNN"
date:   2020-12-01 
author: 단우아범
categories: "언어지능"
tags:	
cover:  "/assets/instacode.png"
---

# 4-1 : RNN 개념
## 1. 모델링이란?
 - 토큰 임베딩, CBOW는 주변의 w개 토큰을 보고 중간에 있는 토근을 알아 맞추는 임베딩 사전학습 방법임
 - 이렇게 학습된 토큰 임베딩은 의미 공간상 비슷한 단어를 비슷한 벡터로 저장함
 - CBOW/Skip-gram으로 학습한 토큰 임베딩은 각 토큰을 고차원의 벡터로 표현하게 되지만, __풀고자하는 NLU 태스크를 직접 풀 수는 없음__ 
 - NLU 태스크를 풀기 위해서는 자연어 문장으로부터 문맥적인 정보를 추출하고, 그를 이용해 태스크를 수행하는 __모델링__ 이 필요함  
 
 ### 모델링이란
  1. 내가 풀고자 하는 태스크를 풀기 위해 Deep Learning 아키텍처를 디자인하는 것
    - 회귀분석 : 아웃풋이 숫자(연속형)
      - Loss : Mean Squared Error, Mean Absolute Error Loss
      - Ground Truth y : '수치'를 나타내는 데이터형(floa32 등)의 벡터
    - 분류 : 아웃풋이 카테고리
      - Loss : Sparse Categorical Cross Entropy Loss / Categorical Cross Entropy Loss
      - Ground Truth y : '클래스'를 나타내는 번호(인덱스) / one-hot vector
      
  2. __태스크의 인풋 & 아웃풋__ 을 디자인하는 것이 중요하다
  
  3. 예측하고자 하는 대상에 대해 좋은 representation (hidden state)를 만드는 것이 핵심!


  - 그렇다면, 토큰의 시퀀스로 이루어져 있는 자연어 문장에서 문맥적인 의미를 담은 `representation(hidden state)`을 만드는 알고리즘은 무엇인가?
    - RNN, LSTM, GRU 등
  
  > Remember 3가지
    >> 자연어의 문맥적 의미는 h 차원의 벡터로 표현된다  
    >> 정보의 가공은 weight matrix를 통해 이루어진다  
    >> Weight matrix의 값은 학습을 통해 태스크를 잘 수행할 수 있는 방향으로 정해진다

 
 - <https://wire.lgcns.com/confluence/plugins/servlet/quiz/download/attachments/vTOirwcBBIOSgFt1HuxwKCpswFKFsk-S-BizIlHGfm4CyO0pnMDpwAEN9TsqL6Pxyoq71H1PjZZ_4B7whrDAXiZdzW-M3_Wfw7mdAd_sHcVuiWBLTND7P5EirmSGXI0tIgirRgqUbwH8AlyGB508kTZFG5VbTP21YbUvD367H-OSgixCZUZHrNgpVcq48V8h/1_%EB%AA%A8%EB%8D%B8%EB%A7%81%EC%9D%B4%EB%9E%80.pdf?version=1&modificationDate=1602464403487&api=v2> 
 

---

# 2. RNN 개념
  <https://wire.lgcns.com/confluence/plugins/servlet/quiz/download/attachments/vTOirwcBBIOSgFt1HuxwKCpswFKFsk-S-BizIlHGfm4CyO0pnMDpwAEN9TsqL6Pxyoq71H1PjZZ_4B7whrDAXiZdzW-M3_Wfw7mdAd_sHcWJqQ4wOFjvCLThabi_ZDUcGYYbB7Fit1xE1i4RE8kTPjZFG5VbTP21YbUvD367H-OSgixCZUZHrNgpVcq48V8h/2_RNN%EA%B0%9C%EB%85%90.pdf?version=1&modificationDate=1602464009210&api=v2>
  
