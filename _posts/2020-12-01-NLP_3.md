---
layout: post
title:  "NLP 1일차 : RNN"
date:   2020-12-01 
author: 단우아범
categories: "언어지능"
tags:	
cover:  "/assets/instacode.png"
---

# 4-1 : RNN 개념
## 1. 모델링이란?
 - 토큰 임베딩, CBOW는 주변의 w개 토큰을 보고 중간에 있는 토근을 알아 맞추는 임베딩 사전학습 방법임
 - 이렇게 학습된 토큰 임베딩은 의미 공간상 비슷한 단어를 비슷한 벡터로 저장함
 - CBOW/Skip-gram으로 학습한 토큰 임베딩은 각 토큰을 고차원의 벡터로 표현하게 되지만, __풀고자하는 NLU 태스크를 직접 풀 수는 없음__ 
 - NLU 태스크를 풀기 위해서는 자연어 문장으로부터 문맥적인 정보를 추출하고, 그를 이용해 태스크를 수행하는 __모델링__ 이 필요함  
 
 ### 모델링이란
  1. 내가 풀고자 하는 태스크를 풀기 위해 Deep Learning 아키텍처를 디자인하는 것
    - 회귀분석 : 아웃풋이 숫자(연속형)
      - Loss : Mean Squared Error, Mean Absolute Error Loss
      - Ground Truth y : '수치'를 나타내는 데이터형(floa32 등)의 벡터
    - 분류 : 아웃풋이 카테고리
      - Loss : Sparse Categorical Cross Entropy Loss / Categorical Cross Entropy Loss
      - Ground Truth y : '클래스'를 나타내는 번호(인덱스) / one-hot vector
      
  2. __태스크의 인풋 & 아웃풋__ 을 디자인하는 것이 중요하다
  
  3. 예측하고자 하는 대상에 대해 좋은 representation (hidden state)를 만드는 것이 핵심!


  - 그렇다면, 토큰의 시퀀스로 이루어져 있는 자연어 문장에서 문맥적인 의미를 담은 `representation(hidden state)`을 만드는 알고리즘은 무엇인가?
    - RNN, LSTM, GRU 등
  
  > Remember 3가지  
    >> 자연어의 문맥적 의미는 h 차원의 벡터로 표현된다  
    >> 정보의 가공은 weight matrix를 통해 이루어진다  
    >> Weight matrix의 값은 학습을 통해 태스크를 잘 수행할 수 있는 방향으로 정해진다
 
 <https://wire.lgcns.com/confluence/plugins/servlet/quiz/download/attachments/vTOirwcBBIOSgFt1HuxwKCpswFKFsk-S-BizIlHGfm4CyO0pnMDpwAEN9TsqL6Pxyoq71H1PjZZ_4B7whrDAXiZdzW-M3_Wfw7mdAd_sHcVuiWBLTND7P5EirmSGXI0tIgirRgqUbwH8AlyGB508kTZFG5VbTP21YbUvD367H-OSgixCZUZHrNgpVcq48V8h/1_%EB%AA%A8%EB%8D%B8%EB%A7%81%EC%9D%B4%EB%9E%80.pdf?version=1&modificationDate=1602464403487&api=v2> 
 

---

## 2. RNN 개념
  - RNN 이란 : Recurrent Neural Network
  - 순차적으로 들어오는 Sequential data에 적합
  - 매 시점의 데이터를 누적한 정보를 이용해 모델링
    <img src ="https://user-images.githubusercontent.com/59005950/100698418-b86df380-33db-11eb-93c5-83215311d868.jpg">
  
  <https://wire.lgcns.com/confluence/plugins/servlet/quiz/download/attachments/vTOirwcBBIOSgFt1HuxwKCpswFKFsk-S-BizIlHGfm4CyO0pnMDpwAEN9TsqL6Pxyoq71H1PjZZ_4B7whrDAXiZdzW-M3_Wfw7mdAd_sHcWJqQ4wOFjvCLThabi_ZDUcGYYbB7Fit1xE1i4RE8kTPjZFG5VbTP21YbUvD367H-OSgixCZUZHrNgpVcq48V8h/2_RNN%EA%B0%9C%EB%85%90.pdf?version=1&modificationDate=1602464009210&api=v2>
  
  - RNN 특징
    - Shared Representation
    - 입력/출력의 유연성
  
---
  

# 4-2 : RNN 한계
## 1. RNN은 속도가 느리다
  - RNN 계열의 모델은 Recurrent Unit이 타임스텝별로 들어오는 인풋을 차례대로 처리하기 때문에,  
  이전 step이 모두 계산되어야 현 step을 계산할 수 있음.  
  그렇기 때문에, __긴 문장이 들어오면 문장을 처리하는 시간이 길어짐__ 
  
## 2. Long term dependency problem : 실제로는 많은 스텝 전의 정보를 활용할 수 있는 모델이 아님
  - 많은 스텝 전의 정보를 기억하지 못하며, 이는 언어에서 중요한 long-term dependency와 관련한 중대한 이슈임
  1. Long term dependency란
    - 언어는 그 특성상 아주 예전에 나왔던 단어에 의존성을 가짐  
    오랜 스텝 이전에 등장했던 토큰은 꽤 긴 범위에 거쳐서 이후 단어들에 영향력을 가짐
    
  2. RNN의 BPTT
    - RNN이 이전에 나온 정보를 '잊어버리게 되는 이유'는 Back-propagation throughout time(BPTT)에 있음
    - RNN은 타임스텝에 따라 정보를 누적하고, 누적된 정보에 대해 Loss 함수가 적용되기 때문에,  
    딥러닝에서 학습을 위해 사용하는 back-propagation이 타임스텝 축에 대해 일어나게 됨    
    - 결론적으로, tanh(활성화함수) 함수를 미분한 값이 0과 1사이에 계속 곱해지다 보니,  
    __오랜 과거의 정보에 대한 그라디언트가 소실__ 되어 버리는 것
    
  3. 문제의 해결 방안
    - 그라디언트 폭발 : Gradient clipping이라는 기법을 적용해 비교적 쉽게 해소할 수 있음
    - 그라디언트 소실 : 소실된 그라디언트를 소생시키는 문제는 간단하지 않음. 이를 해결하기 위해 __새로운 RNN계열의 셀 구조__ 들이 등장하며,  
    이 것이 유명한 __LSTM, GRU__ 등의 유닛임
    
    
---
  

# 4-3 : LSTM
## 1. LSTM : Long-Short Term Memory
  - LSTM은 과거의 정보를 잊어버리지 않도록 RNN unit에서 일어나는 일, 즉 수식을 변형하는 것
  - __정보를 기억하거나 잊어버리는 수문__ 이라고 표현할 수 있는 gate를 3개 사용해 현재 단계의 인풋과 이전 메모리의 비율을 조정함
    <img src = "https://user-images.githubusercontent.com/59005950/100700039-da697500-33df-11eb-99af-eb039dc79b74.png">
  
## 2. LSTM Step by Step
  - RNN에서는 hidden state라는 벡터만을 이용해 정보를 저장하고, 업데이트 함.  
  반면, LSTM은 __cell state__ 와 __hidden state__ 라는 두 개의 정보 흐름을 가지고 있음
  __Cell state__ 는 __장기 기억 메모리, Hidden state__ 는 __해당 스텝__ 에서 모델이 아웃풋으로 내보내는 정보 벡터임
  
  <img src = "https://user-images.githubusercontent.com/59005950/100699885-7a72ce80-33df-11eb-9302-3e212b887284.png">
    - 전 스텝의 hidden과 현재 단계의 input을 고려해 forget gate를 계산하고, 시그모이드를 사용해 0과 1사이의 값이 됨
  
  <img src = "https://user-images.githubusercontent.com/59005950/100699886-7b0b6500-33df-11eb-8f3a-18d38b37f1a4.png">
    - 전 스텝의 hidden과 현재 단계의 input을 고려해 input gate를 계산하고, 0과 1사이의 값을 가지게 됨
  
  <img src = "https://user-images.githubusercontent.com/59005950/100699889-7ba3fb80-33df-11eb-8a16-ef6a0db3bee9.png">
    - 전 단계의 cell state와 2단계의 임시 cell state를 결합해 장기 메모리를 업데이트 함
    - 필요 없는 값들은 1의 forget gate로 인해 0으로 조정되고, 필요한 값은 2번 input gate로 인해 장기메모리에 저장함
  
  <img src = "https://user-images.githubusercontent.com/59005950/100699890-7ba3fb80-33df-11eb-94ff-e1069883d7e2.png">
    - 마지막으로 output gate를 계산하여 이번 스텝의 hidden state를 내보내면 LSTM 셀의 역할이 종료됨  
   업데이트된 장기 기억 메모리에서 이번 스텝의 중요도를 계산해 hidden state로 내보냄
   
## 3. LSTM 요약
  <img src = "https://user-images.githubusercontent.com/59005950/100699891-7c3c9200-33df-11eb-9562-8b23a033c4c7.png">
  
  - 결국 LSTM은 
    - cell state와 hidden state를 두어 장기적으로 가지고 가야 할 정보를 저장함
    - 이 때 장기적으로 가지고 갈 정보 / 이번 스텝에서 장기적으로 기억해야 할 정보 / 중요한 정보를 선택하기 위해 gate를 사용함
  - 이러한 방식으로 작동하는 LSTM은 RNN의 __기울기 소실(gradient vanishing) 문제가 완화__ 되는것으로 알려져 있음
    
    
  
